{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MP5rRkbTpnG8",
        "0J3b18EKdzMC",
        "FY40fGHEg9_i",
        "4sbU1aH5kGFE"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usamireko/DiffSinger_colab_notebook_MLo7/blob/beta/DiffSinger_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP5rRkbTpnG8"
      },
      "source": [
        "# _**[DiffSinger](https://github.com/openvpi/DiffSinger)**_\n",
        "_Singing Voice Synthesis via Shallow Diffusion Mechanism (SVS & TTS)_\n",
        "\n",
        "\\\n",
        "____\n",
        "\n",
        "Note:\n",
        "- This notebook will get update semi-frequently based from the feedback or response from users\n",
        "\n",
        "```We refer \"variance\" as \"parameters\" to avoid the confusion```\n",
        "\n",
        "```Use export_mode if only wanting to export your ONNX files and nothing more```\n",
        "\n",
        "\\\n",
        "____\n",
        "\\\n",
        "#### **This notebook is an edited copy of Kei's Diffsinger [colab notebook](https://colab.research.google.com/drive/1kUg9dz8PPH92NfnLZwgq0_9B9an39t1J?usp=sharing)**\n",
        "####**This notebook is maintained by MLo7**\n",
        "\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "Wv0gfI5feBSc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pK8aicf8A2sj",
        "collapsed": true,
        "outputId": "65493a18-a563-4d03-c9db-9e75f07f1202",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "alert(\"Please take in consideration that by using the Muon+Lynxnet2 branch is considered experimental! Stuff CAN break and behave in unexpected way.\");\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import clear_output, Audio, display, HTML, Javascript\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "branch_select = \"muon_lynxnet2\" # @param [\"main\", \"muon_lynxnet2\"]\n",
        "\n",
        "disclaimer = \"\"\"\n",
        "alert(\"Please take in consideration that by using the Muon+Lynxnet2 branch is considered experimental! Stuff CAN break and behave in unexpected way.\");\n",
        "\"\"\"\n",
        "\n",
        "# Alert for Muon+Lynxnet2\n",
        "if(branch_select == \"muon_lynxnet2\"):\n",
        "  display(Javascript(disclaimer))\n",
        "else:\n",
        "  pass\n",
        "\n",
        "def setup_onnx_export():\n",
        "    print(\"ONNX Export Mode Enabled, Installing required components\")\n",
        "    !git clone https://github.com/openvpi/DiffSinger.git -b {branch_select} /content/DiffSinger\n",
        "    !wget -O /content/mini.sh https://repo.anaconda.com/miniconda/Miniconda3-py310_25.1.1-2-Linux-x86_64.sh\n",
        "    !chmod +x /content/mini.sh\n",
        "    !bash /content/mini.sh -b -f -p /usr/local\n",
        "    !conda install -q -y jupyter\n",
        "    !conda install -q -y google-colab -c conda-forge\n",
        "    !python -m ipykernel install --name \"py310\" --user\n",
        "    print(\"installing dependencies for ONNX conversion\")\n",
        "    !pip install -r /content/DiffSinger/requirements-onnx.txt -q -q -q 2>/dev/null\n",
        "    print(\"Installation complete, time to export those ONNX!\")\n",
        "\n",
        "def setup_standard():\n",
        "    if not os.path.exists(\"/content/pretrain_models\"):\n",
        "        os.makedirs(\"/content/pretrain_models\")\n",
        "\n",
        "    !wget https://github.com/MLo7Ghinsan/DiffSinger_colab_notebook_MLo7/releases/download/OU_files/jpn_dict.txt -O /content/jpn_dict.txt\n",
        "    !rm -rf /content/sample_data\n",
        "    !apt-get install aria2\n",
        "    clear_output()\n",
        "    !git clone https://github.com/UtaUtaUtau/nnsvs-db-converter /content/nnsvs-db-converter\n",
        "    !git clone https://github.com/openvpi/DiffSinger.git -b {branch_select} /content/DiffSinger\n",
        "    !git clone https://github.com/openvpi/MakeDiffSinger /content/MakeDiffSinger\n",
        "    !git clone https://github.com/MLo7Ghinsan/ghin_shenanigans /content/ghin_shenanigans\n",
        "    !git clone https://github.com/openvpi/SOME /content/SOME\n",
        "    clear_output()\n",
        "    !pip install torch torchvision torchaudio\n",
        "    clear_output()\n",
        "    !pip install -r /content/DiffSinger/requirements.txt\n",
        "    !pip install -r /content/SOME/requirements.txt\n",
        "    !pip install mido einops\n",
        "    clear_output()\n",
        "    !wget https://github.com/openvpi/vocoders/releases/download/nsf-hifigan-44.1k-hop512-128bin-2024.02/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip -O /content/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip\n",
        "    !wget https://github.com/openvpi/vocoders/releases/download/pc-nsf-hifigan-44.1k-hop512-128bin-2025.02/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.zip -O /content/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.zip\n",
        "    !wget https://github.com/openvpi/DiffSinger/releases/download/v2.1.0/rmvpe.zip -O /content/rmvpe.zip\n",
        "    !wget https://github.com/openvpi/SOME/releases/download/v1.0.0-baseline/0119_continuous128_5spk.zip -O /content/0119_continuous128_5spk.zip\n",
        "    !wget https://github.com/yxlllc/vocal-remover/releases/download/hnsep_240512/hnsep_240512.zip -O /content/DiffSinger/checkpoints/hnsep_240512.zip\n",
        "    !unzip -q /content/DiffSinger/checkpoints/hnsep_240512.zip -d /content/DiffSinger/checkpoints\n",
        "    !unzip -q /content/0119_continuous128_5spk.zip -d /content/DiffSinger/checkpoints/SOME\n",
        "    !unzip -q /content/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.zip -d /content/DiffSinger/checkpoints\n",
        "    !unzip -q /content/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip -d /content/DiffSinger/checkpoints\n",
        "    !unzip -q /content/rmvpe.zip -d /content/DiffSinger/checkpoints\n",
        "    !unzip -q /content/rmvpe.zip -d /content/MakeDiffSinger/variance-temp-solution/assets\n",
        "    !rm /content/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip\n",
        "    !rm /content/rmvpe.zip\n",
        "    !rm /content/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.zip\n",
        "    !rm /content/0119_continuous128_5spk.zip\n",
        "    # !aria2c -d /content/pretrain_models -o acoustic_pretrain.ckpt https://github.com/haru0l/diffsinger_models/releases/download/acoustic/model_ckpt_steps_49000.ckpt\n",
        "    # !aria2c -d /content/pretrain_models -o variance_pretrain.ckpt https://github.com/haru0l/diffsinger_models/releases/download/variance/model_ckpt_steps_51000.ckpt\n",
        "    clear_output()\n",
        "    !pip install --upgrade tensorboard\n",
        "    clear_output()\n",
        "    !pip install protobuf\n",
        "    clear_output()\n",
        "    !pip install onnxruntime\n",
        "    clear_output()\n",
        "    !pip install pydub\n",
        "    clear_output()\n",
        "\n",
        "#@title # Mount Google Drive and Setup\n",
        "export_mode = False # @param {\"type\":\"boolean\"}\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "if export_mode:\n",
        "    setup_onnx_export()\n",
        "else:\n",
        "    setup_standard()\n",
        "\n",
        "clear_output()\n",
        "print(\"setup complete!\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "!git clone https://github.com/MLo7Ghinsan/ghin_shenanigans /content/ghin_shenanigans 2>/dev/null\n",
        "chika_dance = '<img src=\"https://raw.githack.com/MLo7Ghinsan/ghin_shenanigans/main/image_and_gif/chika_dance.gif\"/>'\n",
        "display(HTML(chika_dance))\n",
        "with open(\"/content/ghin_shenanigans/audio/setup_complete.wav\", \"rb\") as f:\n",
        "    setup_complete_sound = f.read()\n",
        "Audio(data=setup_complete_sound, autoplay=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess data for training**"
      ],
      "metadata": {
        "id": "eexZl_OCDmQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Extract Data\n",
        "#@markdown ___\n",
        "%cd /content\n",
        "#@markdown this cell will create a folder name [raw_data] in the root folder of colab (/content) and extract your data into it\n",
        "\n",
        "data_type = \"lab + wav (NNSVS format)\" # @param [\"lab + wav (NNSVS format)\", \"csv + wav (DiffSinger format)\", \"ds (DiffSinger format)\"]\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> The path to your data zip file\n",
        "\n",
        "data_zip_path = \"/content/drive/MyDrive/Diffsinger/Test/Kiritan.zip\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown nnsvs-db-converter settings (lab + wav ONLY)\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> _These values can exceed the amount that's in your data to maximize the segment length or to keep the data as is_\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> This option is necessary for variance's pitch training\n",
        "estimate_midi_option = \"False\" # @param [\"False\", \"True | parselmouth\", \"True | harvest\", \"True | SOME\"]\n",
        "if estimate_midi_option == \"True | parselmouth\":\n",
        "    estimate_midi = True\n",
        "    midi_pitch_ext = \"parselmouth\"\n",
        "elif estimate_midi_option == \"True | harvest\":\n",
        "    estimate_midi = True\n",
        "    midi_pitch_ext = \"harvest\"\n",
        "else:\n",
        "    estimate_midi = False\n",
        "    midi_pitch_ext = None\n",
        "#@markdown <font size=\"-1.5\"> Determine how long it will segment your data to based on silence phoneme placement (seconds)\n",
        "segment_length = 10 #@param {type:\"slider\", min:5, max:35, step:1}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> Determine how many silence phoneme is allowed in the middle of each segment\n",
        "max_silence_phoneme_amount = 2 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "# leaving -S at 60 so max silence can be 60 seconds that exceeds the segment legnth cap idk why///\n",
        "# making the segment length cap at 35 secs because any longer than that would make training goes really slow\n",
        "\n",
        "# my ass dont remember why i made two... i think one is unnecessary extra but mehhh\n",
        "all_shits = \"/content/raw_data\"\n",
        "all_shits_not_wav_n_lab = \"/content/raw_data/diffsinger_db\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import shutil\n",
        "from pydub import AudioSegment\n",
        "import yaml\n",
        "\n",
        "if os.path.exists(\"/content/raw_data\"):\n",
        "    shutil.rmtree(\"/content/raw_data\")\n",
        "\n",
        "if not os.path.exists(all_shits_not_wav_n_lab):\n",
        "  os.makedirs(all_shits_not_wav_n_lab)\n",
        "\n",
        "# using 'if not' bc i edited the wrong section which im also too lazy to fix it <3\n",
        "if not data_type == \"lab + wav (NNSVS format)\":\n",
        "    #changed to 7zip to support more compression types\n",
        "    !7z x \"$data_zip_path\" -o{all_shits_not_wav_n_lab}\n",
        "    for root, dirs, files in os.walk(all_shits):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".lab\"):\n",
        "                file_path = os.path.join(root, filename)\n",
        "                with open(file_path, \"r\") as file:\n",
        "                    file_data = file.read()\n",
        "                file_data = file_data.replace(\"SP\", \"pau\")\n",
        "                file_data = file_data.replace(\"br\", \"AP\")\n",
        "                with open(file_path, \"w\") as file:\n",
        "                    file.write(file_data)\n",
        "\n",
        "else:\n",
        "    !7z x \"$data_zip_path\" -o{all_shits_not_wav_n_lab}\n",
        "\n",
        "\n",
        "# for funny auto dict generator lmao\n",
        "out = \"/content/DiffSinger/dictionaries\"\n",
        "dictionary_files = []\n",
        "dictionary_conf_lines = []\n",
        "\n",
        "def is_excluded(phoneme):\n",
        "    return phoneme in [\"pau\", \"AP\", \"SP\", \"sil\"]\n",
        "\n",
        "lang_config_path = all_shits_not_wav_n_lab +\"/lang_config.yaml\"\n",
        "\n",
        "if not os.path.exists(lang_config_path):\n",
        "    extra_phonemes = []\n",
        "    merged_phoneme_groups = []\n",
        "    all_phonemes = set()\n",
        "\n",
        "    for root, dirs, files in os.walk(all_shits_not_wav_n_lab):\n",
        "        for file in files:\n",
        "            fpath = os.path.join(root, file)\n",
        "    # honestly if people still have whatever/phoneme in their single dict, they shouldnt be doing single dict in the first place\n",
        "            if file.endswith(\".lab\"):\n",
        "                with open(fpath, \"r\") as lab_file:\n",
        "                    for line in lab_file:\n",
        "                        parts = line.strip().split()\n",
        "                        if len(parts) < 3:\n",
        "                            continue\n",
        "                        phoneme = parts[2]\n",
        "                        if \"/\" in phoneme:\n",
        "                            _, phoneme = phoneme.split(\"/\", 1)\n",
        "                        if not is_excluded(phoneme):\n",
        "                            all_phonemes.add(phoneme)\n",
        "\n",
        "            elif file.endswith(\".csv\"):\n",
        "                with open(fpath, \"r\", newline=\"\") as csv_file:\n",
        "                    csv_reader = csv.DictReader(csv_file)\n",
        "                    for row in csv_reader:\n",
        "                        if \"ph_seq\" in row:\n",
        "                            for phoneme in row[\"ph_seq\"].strip().split():\n",
        "                                if \"/\" in phoneme:\n",
        "                                    _, phoneme = phoneme.split(\"/\", 1)\n",
        "                                if not is_excluded(phoneme):\n",
        "                                    all_phonemes.add(phoneme)\n",
        "\n",
        "            elif file.endswith(\".ds\"):\n",
        "                with open(fpath, \"r\") as json_file:\n",
        "                    data = json.load(json_file)\n",
        "                    for entry in data:\n",
        "                        if \"ph_seq\" in entry:\n",
        "                            for phoneme in entry[\"ph_seq\"].strip().split():\n",
        "                                if \"/\" in phoneme:\n",
        "                                    _, phoneme = phoneme.split(\"/\", 1)\n",
        "                                if not is_excluded(phoneme):\n",
        "                                    all_phonemes.add(phoneme)\n",
        "\n",
        "    os.makedirs(out, exist_ok=True)\n",
        "    custom_dict_path = os.path.join(out, \"dictionary-custom.txt\")\n",
        "    dictionary_files.append(custom_dict_path)\n",
        "    dictionary_conf_lines.append(f\"custom: '{custom_dict_path}'\")\n",
        "    with open(custom_dict_path, \"w\", encoding=\"utf-8\") as out_file:\n",
        "        for phoneme in sorted(all_phonemes):\n",
        "            out_file.write(f\"{phoneme}\\t{phoneme}\\n\")\n",
        "    lang_dict = None\n",
        "\n",
        "else:\n",
        "    with open(lang_config_path, \"r\") as yaml_file:\n",
        "        lang_config = yaml.safe_load(yaml_file)\n",
        "\n",
        "    languages = lang_config.get(\"languages\", [])\n",
        "    extra_phonemes = lang_config.get(\"extra_phonemes\", [])\n",
        "    merged_phoneme_groups = lang_config.get(\"merged_phoneme_groups\", [])\n",
        "\n",
        "    lang_dict = {lang: set() for lang in languages}\n",
        "\n",
        "    for folder in os.listdir(all_shits_not_wav_n_lab):\n",
        "        if \".\" in folder:\n",
        "            _, lang_code = folder.rsplit(\".\", 1)\n",
        "            if lang_code not in languages:\n",
        "                continue\n",
        "\n",
        "            phoneme_folder_path = os.path.join(all_shits_not_wav_n_lab, folder)\n",
        "\n",
        "            for root, dirs, files in os.walk(phoneme_folder_path):\n",
        "                for file in files:\n",
        "                    fpath = os.path.join(root, file)\n",
        "\n",
        "                    if data_type == \"lab + wav (NNSVS format)\":\n",
        "                        if file.endswith(\".lab\"):\n",
        "                            with open(fpath, \"r\") as lab_file:\n",
        "                                for line in lab_file:\n",
        "                                    line = line.strip()\n",
        "                                    if not line:\n",
        "                                        continue\n",
        "                                    parts = line.split()\n",
        "                                    if len(parts) < 3:\n",
        "                                        continue\n",
        "                                    phoneme = parts[2]\n",
        "                                    if \"/\" in phoneme:\n",
        "                                        lang_hint, actual_phoneme = phoneme.split(\"/\", 1)\n",
        "                                        if lang_hint in languages and not is_excluded(actual_phoneme):\n",
        "                                            lang_dict[lang_hint].add(actual_phoneme)\n",
        "                                        continue\n",
        "                                    if not is_excluded(phoneme):\n",
        "                                        lang_dict[lang_code].add(phoneme)\n",
        "\n",
        "                    elif data_type == \"csv + wav (DiffSinger format)\":\n",
        "                        if file.endswith(\".csv\"):\n",
        "                            with open(fpath, \"r\", newline=\"\") as csv_file:\n",
        "                                csv_reader = csv.DictReader(csv_file)\n",
        "                                for row in csv_reader:\n",
        "                                    if \"ph_seq\" in row:\n",
        "                                        ph_seq = row[\"ph_seq\"].strip()\n",
        "                                        for phoneme in ph_seq.split():\n",
        "                                            if \"/\" in phoneme:\n",
        "                                                lang_hint, actual_phoneme = phoneme.split(\"/\", 1)\n",
        "                                                if lang_hint in languages and not is_excluded(actual_phoneme):\n",
        "                                                    lang_dict[lang_hint].add(actual_phoneme)\n",
        "                                                continue\n",
        "                                            if not is_excluded(phoneme):\n",
        "                                                lang_dict[lang_code].add(phoneme)\n",
        "\n",
        "                    else:\n",
        "                        if file.endswith(\".ds\"):\n",
        "                            with open(fpath, \"r\") as json_file:\n",
        "                                data = json.load(json_file)\n",
        "                                for entry in data:\n",
        "                                    if \"ph_seq\" in entry:\n",
        "                                        ph_seq = entry[\"ph_seq\"].strip()\n",
        "                                        for phoneme in ph_seq.split():\n",
        "                                            if \"/\" in phoneme:\n",
        "                                                lang_hint, actual_phoneme = phoneme.split(\"/\", 1)\n",
        "                                                if lang_hint in languages and not is_excluded(actual_phoneme):\n",
        "                                                    lang_dict[lang_hint].add(actual_phoneme)\n",
        "                                                continue\n",
        "                                            if not is_excluded(phoneme):\n",
        "                                                lang_dict[lang_code].add(phoneme)\n",
        "\n",
        "    for lang, ph_set in lang_dict.items():\n",
        "        output_path = os.path.join(out, f\"dictionary-{lang}.txt\")\n",
        "        dictionary_files.append(output_path)\n",
        "        dictionary_conf_lines.append(f\"{lang}: '{output_path}'\")\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as out_file:\n",
        "            for phoneme in sorted(ph_set):\n",
        "                out_file.write(f\"{phoneme}\\t{phoneme}\\n\")\n",
        "\n",
        "# used this for check runs\n",
        "#for dicks in dictionary_files:\n",
        "#    print(dicks)\n",
        "\n",
        "# for vowels and consonants.txt.... well adding luquid type for uta's script\n",
        "dict_path = out\n",
        "vowel_types = {\"a\", \"i\", \"u\", \"e\", \"o\", \"N\", \"M\", \"NG\"}\n",
        "liquid_types = {\"y\", \"w\", \"l\", \"r\"} # r for english labels, it should be fine with jp too\n",
        "vowel_data = []\n",
        "consonant_data = []\n",
        "liquid_data = []\n",
        "\n",
        "for dict_path in dictionary_files:\n",
        "    with open(dict_path, \"r\") as f:\n",
        "        for line in f:\n",
        "            phoneme, _ = line.strip().split(\"\\t\")\n",
        "            if phoneme[0] in vowel_types:\n",
        "                vowel_data.append(phoneme)\n",
        "            elif phoneme[0] in liquid_types:\n",
        "                liquid_data.append(phoneme)\n",
        "            else:\n",
        "                consonant_data.append(phoneme)\n",
        "\n",
        "vowel_data.sort()\n",
        "liquid_data.sort()\n",
        "consonant_data.sort()\n",
        "directory = os.path.dirname(dict_path)\n",
        "\n",
        "# make txt for language json file\n",
        "vowel_txt_path = os.path.join(directory, \"vowels.txt\")\n",
        "with open(vowel_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(vowel_data))\n",
        "liquid_txt_path = os.path.join(directory, \"liquids.txt\")\n",
        "with open(liquid_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(liquid_data))\n",
        "consonant_txt_path = os.path.join(directory, \"consonants.txt\")\n",
        "with open(consonant_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(consonant_data))\n",
        "\n",
        "\n",
        "# here's a funny json append\n",
        "with open(vowel_txt_path, \"r\") as f:\n",
        "    vowel_data = f.read().split()\n",
        "with open(liquid_txt_path, \"r\") as f:\n",
        "    liquid_data = f.read().split()\n",
        "with open(consonant_txt_path, \"r\") as f:\n",
        "    consonant_data = f.read().split()\n",
        "liquid_list = {liquid: True for liquid in liquid_data} #temp fix, might need more research about the push in timing'''\n",
        "phones4json = {\"vowels\": vowel_data, \"liquids\": liquid_list}\n",
        "with open(\"/content/nnsvs-db-converter/lang.sample.json\", \"w\") as rawr:\n",
        "    json.dump(phones4json, rawr, indent=4)\n",
        "\n",
        "\n",
        "if data_type == \"lab + wav (NNSVS format)\":\n",
        "    db_converter_script = \"/content/nnsvs-db-converter/db_converter.py\"\n",
        "    for raw_folder_name in os.listdir(all_shits_not_wav_n_lab):\n",
        "        raw_folder_path = os.path.join(all_shits_not_wav_n_lab, raw_folder_name)\n",
        "        if os.path.isdir(raw_folder_path):\n",
        "            if estimate_midi:\n",
        "                !python3 {db_converter_script} -s {max_silence_phoneme_amount} -l {segment_length} -m -c -L \"/content/nnsvs-db-converter/lang.sample.json\" {raw_folder_path}\n",
        "            else:\n",
        "                !python3 {db_converter_script} -s {max_silence_phoneme_amount} -l {segment_length} -L \"/content/nnsvs-db-converter/lang.sample.json\" {raw_folder_path}\n",
        "            !rm -rf {raw_folder_path}/*.wav {raw_folder_path}/*.lab\n",
        "            !mv {raw_folder_path}/diffsinger_db/* {raw_folder_path} 2> /dev/null\n",
        "            !rm -rf {raw_folder_path}/diffsinger_db\n",
        "            if estimate_midi_option == \"True | SOME\":\n",
        "                !python3 /content/SOME/batch_infer.py --model \"/content/DiffSinger/checkpoints/SOME/0119_continuous256_5spk/model_ckpt_steps_100000_simplified.ckpt\" --dataset {raw_folder_path} --overwrite\n",
        "\n",
        "elif data_type == \"ds (DiffSinger format)\":\n",
        "    ds_segment_script = \"/content/ghin_shenanigans/scripts/ds_segmentor.py\"\n",
        "    ds2csv_script = \"/content/MakeDiffSinger/variance-temp-solution/convert_ds.py\"\n",
        "    for raw_folder_name in os.listdir(all_shits_not_wav_n_lab):\n",
        "        raw_folder_path = os.path.join(all_shits_not_wav_n_lab, raw_folder_name)\n",
        "        if os.path.isdir(raw_folder_path):\n",
        "            ds_exp_path = os.path.join(raw_folder_path, \"ds\")\n",
        "            csv_exp_path = os.path.join(raw_folder_path, \"transcriptions.csv\")\n",
        "            !python3 {ds_segment_script} {raw_folder_path} --export_path {ds_exp_path}\n",
        "            !rm -rf {raw_folder_path}/*.ds #clean it cus why not\n",
        "            !python3 {ds2csv_script} ds2csv {ds_exp_path} {csv_exp_path}\n",
        "else:\n",
        "    pass\n",
        "\n",
        "# make it replace the first SP to AP cus it seems like people always forgot about it\n",
        "for root, _, files in os.walk(all_shits_not_wav_n_lab):\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "            with open(file_path, \"r\", newline=\"\") as input_file:\n",
        "                csv_reader = csv.reader(input_file)\n",
        "                data = [row for row in csv_reader]\n",
        "                header = data[0]\n",
        "                if \"ph_seq\" in header:\n",
        "                    ph_seq_index = header.index(\"ph_seq\")\n",
        "                    if len(data) > 1 and len(data[1]) > ph_seq_index:\n",
        "                        data[1][ph_seq_index] = data[1][ph_seq_index].replace(\"SP\", \"AP\", 1)\n",
        "            with open(file_path, \"w\", newline=\"\") as output_file:\n",
        "                csv_writer = csv.writer(output_file)\n",
        "                csv_writer.writerows(data)\n",
        "\n",
        "print(\"extraction complete!\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"I'm also nice enough to convert your data and also write your dictionaries lmao. You are welcome :)\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JsP1TGg2F1g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Edit Config\n",
        "#@markdown ___\n",
        "\n",
        "import re\n",
        "import os\n",
        "import yaml\n",
        "from IPython.display import clear_output\n",
        "import random #for the random test files lmaoz\n",
        "\n",
        "%cd /content\n",
        "clear_output()\n",
        "#@markdown The model type user is training\n",
        "model_type = \"acoustic\" # @param [\"acoustic\", \"variance\"]\n",
        "config_cap = model_type.upper()\n",
        "diffusion_type = \"reflow\" # @param [\"ddpm\", \"reflow\"]\n",
        "#@markdown If you want to train with lynxnet2 (assumed you chose that branch on the start), change it to lynxnet2, this changes it in acoustic, variance and pitch\n",
        "backbone_pick = \"lynxnet2\" #@param [\"lynxnet2\", \"lynxnet\"]\n",
        "diff_accelerator = \"unipc\" # @param [\"ddim\", \"pndm\", \"dpm-solver\", \"unipc\"]\n",
        "loss_type = \"l2\" # @param [\"l1\", \"l2\"]\n",
        "\n",
        "spk_name = [folder_name for folder_name in os.listdir(all_shits_not_wav_n_lab) if os.path.isdir(os.path.join(all_shits_not_wav_n_lab, folder_name))]\n",
        "# i used spk_name for something else cus i forgor now imma just copy and paste it\n",
        "spk_names = [folder_name for folder_name in os.listdir(all_shits_not_wav_n_lab) if os.path.isdir(os.path.join(all_shits_not_wav_n_lab, folder_name))]\n",
        "num_spk = len(spk_name)\n",
        "num_lang = len(dictionary_files)\n",
        "raw_dir = []\n",
        "datasets = []\n",
        "for folder_name in spk_name:\n",
        "    folder_path = os.path.join(all_shits_not_wav_n_lab, folder_name)\n",
        "    raw_dir.append(folder_path)\n",
        "folder_to_id = {folder_name: i for i, folder_name in enumerate(spk_name)}\n",
        "\n",
        "if num_spk == 1:\n",
        "    singer_type = \"SINGLE-SPEAKER\"\n",
        "    use_spk_id = False\n",
        "\n",
        "    for spk_id, (folder_path, speaker_name) in enumerate(zip(raw_dir, spk_name)):\n",
        "        if data_type == \"ds (DiffSinger format)\":\n",
        "            audio_files = [f[:-4] for f in os.listdir(folder_path) if f.endswith(\".ds\")]\n",
        "        else:\n",
        "            audio_files = [f[:-4] for f in os.listdir(folder_path + \"/wavs\") if f.endswith(\".wav\")]\n",
        "        folder_id = folder_to_id.get(speaker_name, -1)\n",
        "        prefixed_audio_files = [f\"{audio_file}\" for audio_file in audio_files]\n",
        "\n",
        "        random_ass_test_files = prefixed_audio_files[:3]\n",
        "\n",
        "        speaker_name, lang_id = os.path.splitext(speaker_name) #tfw i forgot this last time\n",
        "\n",
        "        datasets.append({\n",
        "        \"raw_data_dir\": folder_path,\n",
        "        \"speaker\": speaker_name,\n",
        "        \"spk_id\": 0,\n",
        "        \"language\": \"custom\",\n",
        "        \"test_prefixes\": random_ass_test_files\n",
        "    })\n",
        "else:\n",
        "    singer_type = \"MULTI-SPEAKER\"\n",
        "    use_spk_id = True\n",
        "\n",
        "    for spk_id, (folder_path, speaker_name) in enumerate(zip(raw_dir, spk_name)):\n",
        "        if data_type == \"ds (DiffSinger format)\":\n",
        "            audio_files = [f[:-4] for f in os.listdir(folder_path) if f.endswith(\".ds\")]\n",
        "        else:\n",
        "            audio_files = [f[:-4] for f in os.listdir(folder_path + \"/wavs\") if f.endswith(\".wav\")]\n",
        "        folder_id = folder_to_id.get(speaker_name, -1)\n",
        "        prefixed_audio_files = [f\"{audio_file}\" for audio_file in audio_files]\n",
        "\n",
        "        random_ass_test_files = prefixed_audio_files[:3]\n",
        "\n",
        "        speaker_name, lang_id = os.path.splitext(speaker_name) #tfw i forgot this last time\n",
        "\n",
        "        datasets.append({\n",
        "            \"raw_data_dir\": folder_path,\n",
        "            \"speaker\": speaker_name,\n",
        "            \"spk_id\": spk_id,\n",
        "            \"language\": lang_id.lstrip(\".\") or \"custom\",\n",
        "            \"test_prefixes\": random_ass_test_files\n",
        "        })\n",
        "\n",
        "dictionaries = {}\n",
        "for line in dictionary_conf_lines:\n",
        "    key, value = line.split(\": \", 1)\n",
        "    dictionaries[key] = value.strip(\"'\")\n",
        "\n",
        "#@markdown Shallow Diffusion training\n",
        "use_shallow_diffusion = \"true | gt_val\" # @param [\"false\", \"true | aux_val\", \"true | gt_val\"]\n",
        "if use_shallow_diffusion == \"false\":\n",
        "    shallow = False\n",
        "    gt_shallow = False\n",
        "elif use_shallow_diffusion == \"true | aux_val\":\n",
        "    shallow = True\n",
        "    gt_shallow = False\n",
        "else:\n",
        "    shallow = True\n",
        "    gt_shallow = True\n",
        "\n",
        "#@markdown Half precision, or mixed precision can result in improved performance, achieving speedups on training (from [doc](https://lightning.ai/docs/pytorch/stable/common/trainer.html#precision))\n",
        "# the reason why i dont add 64 is because colab is already dreadfully slow at 32 so yes im leaving it out\n",
        "precision = \"16-mixed\" # @param [\"32-true\", \"bf16-mixed\", \"16-mixed\", \"bf16\", \"16\"]\n",
        "\n",
        "#@markdown User model save path\n",
        "save_dir = \"/content/drive/MyDrive/Diffsinger/Test/acoustic\" #@param {type:\"string\"}\n",
        "\n",
        "binary_save_dir = save_dir + \"/binary\"\n",
        "\n",
        "conf_dir = save_dir\n",
        "\n",
        "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
        "\n",
        "#@markdown Option to use base model for finetuning\n",
        "\n",
        "enable_finetuning = False # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "#@markdown Path to custom base model, leave blank to use [default](https://github.com/haru0l/diffsinger_models) models\n",
        "#wtf haru i just looked at your readme\"\"\"\"\"\n",
        "\n",
        "base_model_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "if enable_finetuning:\n",
        "    pretrain = True\n",
        "    if base_model_path:\n",
        "        pretrain_ckpt = base_model_path\n",
        "    else:\n",
        "      print(\"Downloading pretrained models, please wait!\")\n",
        "      !aria2c -d /content/pretrain_models -o acoustic_pretrain.ckpt https://github.com/haru0l/diffsinger_models/releases/download/acoustic/model_ckpt_steps_49000.ckpt\n",
        "      !aria2c -d /content/pretrain_models -o variance_pretrain.ckpt https://github.com/haru0l/diffsinger_models/releases/download/variance/model_ckpt_steps_51000.ckpt\n",
        "      pretrain_ckpt = f\"/content/pretrain_models/{model_type}_pretrain.ckpt\"\n",
        "      finetune_strict_shapes = False\n",
        "      finetune_ckpt_path = pretrain_ckpt\n",
        "else:\n",
        "    pretrain = False\n",
        "    finetune_strict_shapes = True #default value\n",
        "    finetune_ckpt_path = None #default value\n",
        "\n",
        "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
        "\n",
        "#@markdown Model embeds check; Tension, Energy, Breathiness, Voicing | for both <font color = \"yellow\">acoustic and variance\n",
        "\n",
        "#@markdown <font size=\"-1.5\">  ~~There was not any embed limatation, Lotte said this is wrong~~\n",
        "#@markdown <font size=\"-1.5\">  If you're going to train more than one, they have to be trained together, because there's only one variance.onnx\n",
        "#@markdown <font size=\"-1.5\"> Energy training is no longer recommended and might not even be supported in the future\n",
        "tension_training = True # @param {type:\"boolean\"}\n",
        "energy_training = False # @param {type:\"boolean\"}\n",
        "breathiness_training = True # @param {type:\"boolean\"}\n",
        "voicing_training = True # @param {type:\"boolean\"}\n",
        "\n",
        "\n",
        "parameter_extraction_method = \"vr\" # @param [\"vr\", \"world\"]\n",
        "\n",
        "### forcing data aug to be true by default cus i dont think anyone would disable it and its good to be on by default\n",
        "data_aug = True #param {type:\"boolean\"}\n",
        "\n",
        "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
        "\n",
        "#@markdown Model training check | for <font color = \"yellow\">variance </font>only\n",
        "\n",
        "\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> due to skill issues, if user wish to train with glide embed, please enable it manually in the config\n",
        "pitch_training = \"False\" # @param [\"False\", \"True | Standard\", \"True | MelodyEncoder\"]\n",
        "if pitch_training == \"False\":\n",
        "    pitch_training = False\n",
        "    use_melody_encoder = False\n",
        "    use_glide_embed = False\n",
        "elif pitch_training == \"True | Standard\":\n",
        "    pitch_training = True\n",
        "    use_melody_encoder = False\n",
        "    use_glide_embed = False\n",
        "else:\n",
        "    pitch_training = True\n",
        "    use_melody_encoder = True\n",
        "    use_glide_embed = False\n",
        "\n",
        "duration_training = True #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
        "\n",
        "#@markdown Pitch extractor algorithm\n",
        "\n",
        "f0_ext = \"rmvpe\" # @param [\"parselmouth\", \"rmvpe\", \"harvest\"]\n",
        "if f0_ext == \"rmvpe\":\n",
        "    pe_ckpt_pth = \"checkpoints/rmvpe/model.pt\"\n",
        "else:\n",
        "    pe_ckpt_pth = None\n",
        "\n",
        "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
        "\n",
        "#@markdown Proceeding sections are the parameters that will greatly affect the model's final quality and size. Read about them [here](https://github.com/openvpi/DiffSinger/blob/main/docs/ConfigurationSchemas.md)\n",
        "\n",
        "#@markdown So if you don't know what they do then please <font color = \"red\">leave these options at default </font>, otherwise it could affect your model badly\n",
        "\n",
        "#@markdown <font size=\"-2.5\">anyone is welcome to experiment though\n",
        "\n",
        "#@markdown <font size=\"-1.5\">model_hidden_size: hidden layers for FS2 and token param embeds\n",
        "\n",
        "#@markdown <font size=\"-1.5\">model_residual_layers | model_residual_channels: the model's main layers and channels\n",
        "\n",
        "#@markdown ....................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
        "#@markdown Model's network/layer size for <font color = \"yellow\">acoustic\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> The quality of samplig_algorithm is in order, range from euler being the LEAST accurate to rk5 being the MOST accurate.... Though euler works fine on most cases\n",
        "sampling_algorithm = \"euler\" # @param [\"euler\", \"rk2\", \"rk4\", \"rk5\"]\n",
        "\n",
        "acoustic_hidden_size = 256 # @param {type:\"slider\", min:2, max:1024, step:2}\n",
        "\n",
        "acoustic_num_layers = 6 # @param {type:\"slider\", min:2, max:42, step:2}\n",
        "acoustic_num_channels = 1024 # @param {type:\"slider\", min:2, max:2048, step:2}\n",
        "\n",
        "#@markdown Model's network/layer size for <font color = \"yellow\">variance\n",
        "variance_hidden_size = 256 # @param {type:\"slider\", min:2, max:1024, step:2}\n",
        "duration_hidden_size = 512 # @param {type:\"slider\", min:2, max:1024, step:2}\n",
        "melody_encoder_hidden_size = 128 # @param {type:\"slider\", min:2, max:1024, step:2}\n",
        "\n",
        "pitch_num_layers = 6 # @param {type:\"slider\", min:2, max:100, step:2}\n",
        "pitch_num_channels = 512 # @param {type:\"slider\", min:2, max:1024, step:2}\n",
        "variance_num_layers = 6 # @param {type:\"slider\", min:2, max:100, step:2}\n",
        "variance_num_channels = 384 # @param {type:\"slider\", min:2, max:1024, step:2}\n",
        "\n",
        "\n",
        "\n",
        "with open(\"/content/DiffSinger/configs/base.yaml\", \"r\") as config:\n",
        "    mother = yaml.safe_load(config)\n",
        "mother[\"pl_trainer_precision\"] = precision\n",
        "with open(\"/content/DiffSinger/configs/base.yaml\", \"w\") as config:\n",
        "    yaml.dump(mother, config)\n",
        "\n",
        "if  data_type == \"ds (DiffSinger format)\":\n",
        "    prefer_ds = True\n",
        "else:\n",
        "    prefer_ds = False\n",
        "\n",
        "if model_type == \"acoustic\":\n",
        "    with open(\"/content/DiffSinger/configs/acoustic.yaml\", \"r\") as config:\n",
        "        bitch_ass_config = yaml.safe_load(config)\n",
        "    bitch_ass_config[\"datasets\"] = datasets\n",
        "    bitch_ass_config[\"num_spk\"] = num_spk\n",
        "    bitch_ass_config[\"use_spk_id\"] = use_spk_id\n",
        "    bitch_ass_config[\"extra_phonemes\"] = extra_phonemes\n",
        "    bitch_ass_config[\"merged_phoneme_groups\"] = merged_phoneme_groups\n",
        "    bitch_ass_config[\"use_lang_id\"] = bool(merged_phoneme_groups)\n",
        "    bitch_ass_config[\"num_lang\"] = num_lang\n",
        "    bitch_ass_config[\"pretrain\"] = pretrain\n",
        "    bitch_ass_config[\"diffusion_type\"] = diffusion_type\n",
        "    bitch_ass_config[\"diff_accelerator\"] = diff_accelerator\n",
        "    bitch_ass_config[\"main_loss_type\"] = loss_type\n",
        "    bitch_ass_config[\"binary_data_dir\"] = binary_save_dir\n",
        "    bitch_ass_config[\"dictionaries\"] = dictionaries\n",
        "    bitch_ass_config[\"augmentation_args\"][\"random_pitch_shifting\"][\"enabled\"] = data_aug\n",
        "    bitch_ass_config[\"augmentation_args\"][\"random_time_stretching\"][\"enabled\"] = data_aug\n",
        "    bitch_ass_config[\"use_key_shift_embed\"] = data_aug\n",
        "    bitch_ass_config[\"use_speed_embed\"] = data_aug\n",
        "    bitch_ass_config[\"pe\"] = f0_ext\n",
        "    bitch_ass_config[\"use_energy_embed\"] = energy_training\n",
        "    bitch_ass_config[\"use_breathiness_embed\"] = breathiness_training\n",
        "    bitch_ass_config[\"use_tension_embed\"] = tension_training\n",
        "    bitch_ass_config[\"use_voicing_embed\"] = voicing_training\n",
        "\n",
        "    bitch_ass_config[\"pe_ckpt\"] = pe_ckpt_pth\n",
        "    bitch_ass_config[\"tension_smooth_width\"] = 0.06 #0.12\n",
        "    #shallow diff stuff\n",
        "    bitch_ass_config[\"use_shallow_diffusion\"] = shallow\n",
        "    bitch_ass_config[\"shallow_diffusion_args\"][\"val_gt_start\"] = gt_shallow\n",
        "    #finetue stuff\n",
        "    bitch_ass_config[\"finetune_enabled\"] = enable_finetuning\n",
        "    bitch_ass_config[\"finetune_ckpt_path\"] = finetune_ckpt_path\n",
        "    bitch_ass_config[\"finetune_strict_shapes\"] = finetune_strict_shapes\n",
        "    #vr\n",
        "    bitch_ass_config[\"hnsep\"] = parameter_extraction_method\n",
        "    #layers\n",
        "    bitch_ass_config[\"sampling_algorithm\"] = sampling_algorithm\n",
        "    bitch_ass_config[\"hidden_size\"] = acoustic_hidden_size\n",
        "    bitch_ass_config[\"backbone_type\"] = backbone_pick\n",
        "    bitch_ass_config[\"backbone_args\"][\"num_layers\"] = acoustic_num_layers\n",
        "    bitch_ass_config[\"backbone_args\"][\"num_channels\"] = acoustic_num_channels\n",
        "\n",
        "    with open(\"/content/DiffSinger/configs/acoustic.yaml\", \"w\") as config:\n",
        "        yaml.dump(bitch_ass_config, config)\n",
        "else:\n",
        "    with open(\"/content/DiffSinger/configs/variance.yaml\", \"r\") as config:\n",
        "        bitch_ass_config = yaml.safe_load(config)\n",
        "    bitch_ass_config[\"datasets\"] = datasets\n",
        "    bitch_ass_config[\"num_spk\"] = num_spk\n",
        "    bitch_ass_config[\"use_spk_id\"] = use_spk_id\n",
        "    bitch_ass_config[\"extra_phonemes\"] = extra_phonemes\n",
        "    bitch_ass_config[\"merged_phoneme_groups\"] = merged_phoneme_groups\n",
        "    bitch_ass_config[\"use_lang_id\"] = bool(merged_phoneme_groups)\n",
        "    bitch_ass_config[\"num_lang\"] = num_lang\n",
        "    bitch_ass_config[\"main_loss_type\"] = loss_type\n",
        "    bitch_ass_config[\"diffusion_type\"] = diffusion_type\n",
        "    bitch_ass_config[\"diff_accelerator\"] = diff_accelerator\n",
        "    bitch_ass_config[\"binary_data_dir\"] = binary_save_dir\n",
        "    bitch_ass_config[\"dictionaries\"] = dictionaries\n",
        "    bitch_ass_config[\"pe\"] = f0_ext # i think variance uses it for pitch ref as ground-truth for pitch training soooo\n",
        "    bitch_ass_config[\"pe_ckpt\"] = pe_ckpt_pth #same goes to this one\n",
        "    bitch_ass_config[\"tension_smooth_width\"] = 0.06 #0.12\n",
        "\n",
        "    bitch_ass_config[\"predict_energy\"] = energy_training\n",
        "    bitch_ass_config[\"predict_breathiness\"] = breathiness_training\n",
        "    bitch_ass_config[\"predict_tension\"] = tension_training\n",
        "    bitch_ass_config[\"predict_pitch\"] = pitch_training\n",
        "    bitch_ass_config[\"predict_voicing\"] = voicing_training\n",
        "\n",
        "    bitch_ass_config[\"use_melody_encoder\"] = use_melody_encoder\n",
        "    bitch_ass_config[\"use_glide_embed\"] = use_glide_embed\n",
        "    bitch_ass_config[\"predict_dur\"] = duration_training\n",
        "    bitch_ass_config[\"binarization_args\"][\"prefer_ds\"] = prefer_ds\n",
        "    #finetune stuff\n",
        "    bitch_ass_config[\"finetune_enabled\"] = enable_finetuning\n",
        "    bitch_ass_config[\"finetune_ckpt_path\"] = finetune_ckpt_path\n",
        "    bitch_ass_config[\"finetune_strict_shapes\"] = finetune_strict_shapes\n",
        "    #vr\n",
        "    bitch_ass_config[\"hnsep\"] = parameter_extraction_method\n",
        "    bitch_ass_config[\"hnsep_ckpt\"] = \"checkpoints/vr/model.pt\"\n",
        "    #layers\n",
        "    bitch_ass_config[\"hidden_size\"] = variance_hidden_size\n",
        "    bitch_ass_config[\"dur_prediction_args\"][\"hidden_size\"] = duration_hidden_size\n",
        "    bitch_ass_config[\"melody_encoder_args\"][\"hidden_size\"] = melody_encoder_hidden_size\n",
        "    bitch_ass_config[\"variances_prediction_args\"][\"backbone_type\"] = backbone_pick\n",
        "    bitch_ass_config[\"variances_prediction_args\"][\"backbone_args\"][\"num_layers\"] = variance_num_layers\n",
        "    bitch_ass_config[\"variances_prediction_args\"][\"backbone_args\"][\"num_channels\"] = variance_num_channels\n",
        "    bitch_ass_config[\"pitch_prediction_args\"][\"backbone_type\"] = backbone_pick\n",
        "    bitch_ass_config[\"pitch_prediction_args\"][\"backbone_args\"][\"num_layers\"] = pitch_num_layers\n",
        "    bitch_ass_config[\"pitch_prediction_args\"][\"backbone_args\"][\"num_channels\"] = pitch_num_channels\n",
        "\n",
        "    with open(\"/content/DiffSinger/configs/variance.yaml\", \"w\") as config:\n",
        "        yaml.dump(bitch_ass_config, config)\n",
        "\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "with open(\"/content/DiffSinger/utils/hparams.py\", \"r\") as f:\n",
        "    hparams_py_read = f.read()\n",
        "hparams_py_read = re.sub(r\"args_work_dir\\s*=\\s*.*\", f\"args_work_dir = '{save_dir}'\", hparams_py_read)\n",
        "with open(\"/content/DiffSinger/utils/hparams.py\", \"w\") as f:\n",
        "    f.write(hparams_py_read)\n",
        "\n",
        "with open(\"/content/DiffSinger/utils/training_utils.py\", \"r\") as f:\n",
        "    training_utils_stuff = f.read()\n",
        "training_utils_stuff = re.sub(\"relative_path\\s*=\\s*.*\", \"relative_path = filepath.relative_to(Path('/content').resolve())\", training_utils_stuff)\n",
        "with open(\"/content/DiffSinger/utils/training_utils.py\", \"w\") as f:\n",
        "    f.write(training_utils_stuff)\n",
        "\n",
        "spk_names = [os.path.splitext(name)[0] for name in spk_names]\n",
        "dict_dir = os.path.dirname(dict_path)\n",
        "\n",
        "print(\"config updated! see below for config's information\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(f\"+++---{config_cap} {singer_type} TRAINING---+++\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"|\")\n",
        "print(\"+++---user's settings---+++\")\n",
        "print(\"\\n\")\n",
        "print(f\"speaker name: {spk_names}\")\n",
        "print(\"\\n\")\n",
        "print(f\"data augmentation: {data_aug}\")\n",
        "print(\"\\n\")\n",
        "print(f\"pitch extractor: {f0_ext}\")\n",
        "print(\"\\n\")\n",
        "print(f\"backbone: {backbone_pick}\")\n",
        "print(\"\\n\")\n",
        "print(f\"binary data save directory: {binary_save_dir}\")\n",
        "print(\"\\n\")\n",
        "print(f\"your model will be saved to: {save_dir}\")\n",
        "print(\"\\n\")\n",
        "print(\"==========================================================================================\")\n",
        "print(\"\\n\")\n",
        "print(\"+++---other auto-defined settings---+++\")\n",
        "#print(\"\\n\")\n",
        "#print(f\"test files (auto selected): {random_ass_test_files}\")\n",
        "print(\"\\n\")\n",
        "print(f\"dictionary (auto generated): {dict_dir} (check this directory)\")\n",
        "print(\"\\n\")\n",
        "print(\"==========================================================================================\")\n",
        "print(\"\\n\")\n",
        "print(\"if you don't like or disagree with any of these options,\")\n",
        "print(f\"you can go and edit the config at [/content/DiffSinger/configs/{model_type}.yaml]\")\n"
      ],
      "metadata": {
        "id": "nI3dzDv_Mr9Y",
        "cellView": "form",
        "outputId": "5e7a6ee6-98e8-4099-f8a0-abef9c77f4cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'all_shits_not_wav_n_lab' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-2173503677.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"l2\"\u001b[0m \u001b[0;31m# @param [\"l1\", \"l2\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mspk_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_shits_not_wav_n_lab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_shits_not_wav_n_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m# i used spk_name for something else cus i forgor now imma just copy and paste it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mspk_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfolder_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_shits_not_wav_n_lab\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_shits_not_wav_n_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfolder_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'all_shits_not_wav_n_lab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Preprocess data\n",
        "\n",
        "#we dont need that old f0 limit change anymore <3\n",
        "training_config = f\"/content/DiffSinger/configs/{model_type}.yaml\"\n",
        "%cd /content/DiffSinger\n",
        "!python /content/DiffSinger/scripts/binarize.py --config {training_config} --reset"
      ],
      "metadata": {
        "cellView": "form",
        "id": "76NvDR1cXlDM",
        "collapsed": true,
        "outputId": "757a1001-8aa2-40ce-f579-8aa798779b91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_type' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-3491083838.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#we dont need that old f0 limit change anymore <3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtraining_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"/content/DiffSinger/configs/{model_type}.yaml\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/DiffSinger'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python /content/DiffSinger/scripts/binarize.py --config {training_config} --reset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_type' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "0J3b18EKdzMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #Train your model\n",
        "%cd /content/DiffSinger\n",
        "import re\n",
        "import os\n",
        "import yaml\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> Step interval of when your model will be validate and save\n",
        "save_interval = 2000 #@param {type:\"slider\", min:100, max:10000, step:100}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> batch size setting, too low can cause bottleneck, too high can cause oom\n",
        "batch_size = 8 # @param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> step interval of when your model will stop training automatically\n",
        "max_updates = 160000 # @param {type:\"slider\", min:100, max:2000000, step:100}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown ###**Only edit this section if you want to resume training**\n",
        "resume_training = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> select this option if you locally binarized your data | this option will only append your binary data path in your config | \"binary\" folder must be in the same directory as config.yaml\n",
        "local_data = False #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> path to the config you got from training\n",
        "re_config_path = \"\" #@param {type:\"string\"}\n",
        "model_dir = os.path.dirname(re_config_path)\n",
        "save_dir = model_dir\n",
        "if resume_training:\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"r\") as f:\n",
        "        hparams_py_read = f.read()\n",
        "    hparams_py_read = re.sub(r\"args_work_dir\\s*=\\s*.*\", f\"args_work_dir = '{save_dir}'\", hparams_py_read)\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"w\") as f:\n",
        "        f.write(hparams_py_read)\n",
        "    with open(\"/content/DiffSinger/utils/training_utils.py\", \"r\") as f:\n",
        "        training_utils_stuff = f.read()\n",
        "    training_utils_stuff = re.sub(\"relative_path\\s*=\\s*.*\", \"relative_path = filepath.relative_to(Path('/content').resolve())\", training_utils_stuff)\n",
        "    with open(\"/content/DiffSinger/utils/training_utils.py\", \"w\") as f:\n",
        "        f.write(training_utils_stuff)\n",
        "\n",
        "    config_path = re_config_path\n",
        "    log_dir = save_dir\n",
        "\n",
        "    !cp {model_dir}/*.txt /content/DiffSinger/dictionaries\n",
        "\n",
        "else:\n",
        "    config_path = training_config\n",
        "    log_dir = conf_dir\n",
        "\n",
        "with open(config_path, \"r\") as config:\n",
        "    ehe = yaml.safe_load(config)\n",
        "config_dir = os.path.dirname(config_path)\n",
        "yuh = os.path.join(config_dir, \"binary\")\n",
        "\n",
        "ehe[\"val_check_interval\"] = save_interval\n",
        "ehe[\"max_batch_size\"] = batch_size\n",
        "ehe[\"max_updates\"] = max_updates\n",
        "if local_data:\n",
        "    ehe[\"binary_data_dir\"] = yuh\n",
        "with open(config_path, \"w\") as config:\n",
        "    yaml.dump(ehe, config)\n",
        "\n",
        "logs = log_dir\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {logs}/lightning_logs\n",
        "\n",
        "!python /content/DiffSinger/scripts/train.py --config {config_path} --exp_name ${save_dir} --reset"
      ],
      "metadata": {
        "id": "Lu5w72UWgccC",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Convert model to ONNX format**"
      ],
      "metadata": {
        "id": "FY40fGHEg9_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Drop Speakers from Model (Optional)\n",
        "#@markdown ___\n",
        "#@markdown <font size=\"-1.5\"> Use this to drop speakers from your model for distribution. You will need to do it for both acoustic and variance models.\n",
        "\n",
        "drop_model_path = '' #@param {type: \"string\"}\n",
        "#@markdown <font size=\"-1.5\"> Type the ID of speakers you'd like to KEEP separated by commas. Ex: \"0,3,4\" <br>\n",
        "#@markdown <font size=\"-1.5\"> Note: You can find the ID of speakers in the model by opening the ```spk_map.json``` file in the model folder.<br>\n",
        "#@markdown <font size=\"-1.5\"> If you see ```{\"natural\": 0, \"power\": 1, \"silly\": 2}``` but only want to keep \"natural\" and \"power\", type ```0,1``` below.\n",
        "retain_speakers = '' #@param {type: \"string\"}\n",
        "#@markdown <font size=\"-1.5\"> If you don't know what this means, don't change it.\n",
        "fill_embed = 'zeros' #@param ['zeros', 'random', 'mean', 'cyclic']\n",
        "\n",
        "drop_out_path = drop_model_path[:-5] + '_spk-dropped.ckpt'\n",
        "\n",
        "!python /content/DiffSinger/scripts/drop_spk.py {drop_model_path} {drop_out_path} --retain {retain_speakers} --fill {fill_embed}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "21ILzW4OEnh4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Export ONNX\n",
        "#@markdown ___\n",
        "%cd /content\n",
        "from IPython.display import clear_output\n",
        "clear_output()\n",
        "import os\n",
        "import zipfile\n",
        "import shutil\n",
        "\n",
        "\n",
        "if export_mode:\n",
        "  pass\n",
        "else:\n",
        "    print(\"Installing components to make ONNX work\")\n",
        "    print(\"installing dependencies for ONNX conversion\")\n",
        "    !pip install -r /content/DiffSinger/requirements-onnx.txt --quiet\n",
        "    print(\"Installation complete, time to export those ONNX!\")\n",
        "# to counter IF the user is to re-run this cell <3\n",
        "if os.path.exists(\"/content/OU_compatible_files\"):\n",
        "    shutil.rmtree(\"/content/OU_compatible_files\")\n",
        "    os.remove(\"/content/jpn_dict.txt\")\n",
        "else:\n",
        "    pass\n",
        "\n",
        "#@markdown  select this if you don't want to see the onnx converter's output\n",
        "no_output = True # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown  path to your **ACOUSTIC CHECKPOINT** (leave blank if you don't have any): automatically use latest checkpoint that is in the same folder\n",
        "acoustic_checkpoint_path = \"\" #@param{type:\"string'}\n",
        "acoustic_folder_name = os.path.basename(os.path.dirname(acoustic_checkpoint_path)) + \"_acoustic\"\n",
        "acoustic_folder_path = os.path.dirname(acoustic_checkpoint_path)\n",
        "\n",
        "#@markdown  path to your **VARIANCE CHECKPOINT** (leave blank if you don't have any): automatically use latest checkpoint that is in the same folder\n",
        "variance_checkpoint_path = \"\" #@param{type:\"string'}\n",
        "variance_folder_name = os.path.basename(os.path.dirname(variance_checkpoint_path)) + \"_variance\"\n",
        "variance_folder_path = os.path.dirname(variance_checkpoint_path)\n",
        "\n",
        "#@markdown  path to where you want to save your ONNX files (it will create a folder named \"onnx\" in this path)\n",
        "exp_folder = \"\" #@param{type:\"string\"}\n",
        "\n",
        "acoustic_onnx_exp = exp_folder + \"/onnx/acoustic\"\n",
        "variance_onnx_exp = exp_folder + \"/onnx/variance\"\n",
        "\n",
        "if not acoustic_checkpoint_path:\n",
        "    print(\"\\n\")\n",
        "    print(\"acoustic ckeckpoint path not specified, not exporting acoustic ONNX...\")\n",
        "else:\n",
        "    print(\"\\n\")\n",
        "    print(\"converting acoustic to onnx...\")\n",
        "    #cp stuff cus apparently exporter doesnt work without it\n",
        "    !cp {acoustic_folder_path}/config.yaml -r /content/DiffSinger/checkpoints/{acoustic_folder_name}\n",
        "    search_text = \"        args_work_dir = os.path.join(\"\n",
        "    replacement = f\"        args_work_dir = '{acoustic_folder_path}'\"\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_text in line:\n",
        "            lines[i] = replacement + \"\\n\"\n",
        "            break\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"w\") as file:\n",
        "            file.writelines(lines)\n",
        "    #incase if anyone wanna change it lmao\n",
        "    search_text_alt = \"        args_work_dir = '\"\n",
        "    replacement_alt = f\"        args_work_dir = '{acoustic_folder_path}'\"\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_text_alt in line:\n",
        "            lines[i] = replacement_alt + \"\\n\"\n",
        "            break\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"w\") as file:\n",
        "            file.writelines(lines)\n",
        "\n",
        "    if no_output:\n",
        "        !python /content/DiffSinger/scripts/export.py acoustic --exp {acoustic_folder_name} --out {exp_folder}/onnx/acoustic >/dev/null 2>&1\n",
        "    else:\n",
        "        !python /content/DiffSinger/scripts/export.py acoustic --exp {acoustic_folder_name} --out {exp_folder}/onnx/acoustic\n",
        "\n",
        "\n",
        "if not variance_checkpoint_path:\n",
        "    print(\"\\n\")\n",
        "    print(\"variance ckeckpoint path not specified, not exporting variance ONNX...\")\n",
        "else:\n",
        "    print(\"\\n\")\n",
        "    print(\"converting variance to onnx...\")\n",
        "    #cp stuff cus apparently exporter doesnt work without it\n",
        "    !cp {variance_folder_path}/config.yaml -r /content/DiffSinger/checkpoints/{variance_folder_name}\n",
        "    search_text = \"        args_work_dir = os.path.join(\"\n",
        "    replacement = f\"        args_work_dir = '{variance_folder_path}'\"\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_text in line:\n",
        "            lines[i] = replacement + \"\\n\"\n",
        "            break\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"w\") as file:\n",
        "            file.writelines(lines)\n",
        "    #incase if anyone wanna change it lmao\n",
        "    search_text_alt = \"        args_work_dir = '\"\n",
        "    replacement_alt = f\"        args_work_dir = '{variance_folder_path}'\"\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"r\") as file:\n",
        "        lines = file.readlines()\n",
        "    for i, line in enumerate(lines):\n",
        "        if search_text_alt in line:\n",
        "            lines[i] = replacement_alt + \"\\n\"\n",
        "            break\n",
        "    with open(\"/content/DiffSinger/utils/hparams.py\", \"w\") as file:\n",
        "            file.writelines(lines)\n",
        "    if no_output:\n",
        "        !python /content/DiffSinger/scripts/export.py variance --exp {variance_folder_name} --out {exp_folder}/onnx/variance >/dev/null 2>&1\n",
        "    else:\n",
        "        !python /content/DiffSinger/scripts/export.py variance --exp {variance_folder_name} --out {exp_folder}/onnx/variance\n",
        "\n",
        "\n",
        "if not variance_checkpoint_path:\n",
        "    folder_paths = [acoustic_onnx_exp]\n",
        "elif not acoustic_checkpoint_path:\n",
        "    folder_paths = [variance_onnx_exp]\n",
        "else:\n",
        "    folder_paths = [acoustic_onnx_exp, variance_onnx_exp]\n",
        "\n",
        "patterns = {\"acoustic.onnx\": \"acoustic.onnx\", \"dur.onnx\": \"dur.onnx\", \"linguistic.onnx\": \"linguistic.onnx\", \"pitch.onnx\": \"pitch.onnx\", \"variance.onnx\": \"variance.onnx\", \"phonemes.txt\": \"phonemes.txt\"}\n",
        "\n",
        "for folder_path in folder_paths:\n",
        "    for filename in os.listdir(folder_path):\n",
        "        for pattern, new_name in patterns.items():\n",
        "            if pattern in filename:\n",
        "                old_path = os.path.join(folder_path, filename)\n",
        "                new_path = os.path.join(folder_path, new_name)\n",
        "                if os.path.exists(old_path):\n",
        "                    os.rename(old_path, new_path)\n",
        "for folder_path in folder_paths:\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if \"acoustic_acoustic.\" in filename:\n",
        "            new_filename = filename.replace(\"acoustic_acoustic.\", \"acoustic_\")\n",
        "        elif \"variance_variance.\" in filename:\n",
        "            new_filename = filename.replace(\"variance_variance.\", \"variance_\")\n",
        "        else:\n",
        "            new_filename = filename\n",
        "        old_path = os.path.join(folder_path, filename)\n",
        "        new_path = os.path.join(folder_path, new_filename)\n",
        "        os.rename(old_path, new_path)\n",
        "print(\"\\n\")\n",
        "print(\"ONNX export complete! Please refer to https://github.com/xunmengshe/OpenUtau/wiki/Voicebank-Development to make your model OU compatible\")\n",
        "print(\"\\n\")\n",
        "print(\"Or use the 'Build OpenUtau VB' cell to have things set up for you\")\n"
      ],
      "metadata": {
        "id": "x33iZhZchEMW",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Build OpenUtau VB\n",
        "#@markdown ___\n",
        "# Inspired script by MLo7Ghinsan: https://github.com/MLo7Ghinsan/ghin_shenanigans/blob/main/scripts/voicebank_exporter.py\n",
        "import shutil\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Enter the folders and voicebank name:\n",
        "acoustic_onnx_folder = \"\"  #@param{type:\"string\"}\n",
        "variance_onnx_folder = \"\"  #@param{type:\"string\"}\n",
        "exp_folder = \"\"            #@param{type:\"string\"}\n",
        "name = \"\"                  #@param{type:\"string\"}\n",
        "\n",
        "# VB Type (hardcoded now)\n",
        "dictionary_type = \"multi-dict\"\n",
        "\n",
        "PHONEME_TYPES = {\n",
        "    \"a\": \"vowel\", \"i\": \"vowel\", \"u\": \"vowel\", \"e\": \"vowel\", \"o\": \"vowel\",\n",
        "    \"b\": \"stop\", \"d\": \"stop\", \"g\": \"stop\", \"k\": \"stop\", \"p\": \"stop\", \"q\": \"stop\", \"t\": \"stop\",\n",
        "    \"c\": \"affricate\", \"j\": \"affricate\",\n",
        "    \"f\": \"fricative\", \"h\": \"fricative\", \"s\": \"fricative\", \"v\": \"fricative\", \"z\": \"fricative\",\n",
        "    \"l\": \"liquid\", \"r\": \"liquid\",\n",
        "    \"m\": \"nasal\", \"n\": \"nasal\",\n",
        "    \"w\": \"semivowel\", \"y\": \"semivowel\",\n",
        "}\n",
        "\n",
        "class VoicebankBuilder:\n",
        "    def __init__(self, acoustic_folder, variance_folder, output_folder, vb_name, dict_type):\n",
        "        self.acoustic_path = Path(acoustic_folder)\n",
        "        self.variance_path = Path(variance_folder)\n",
        "        self.output_path = Path(output_folder)\n",
        "        self.vb_name = vb_name\n",
        "        self.dict_type = dict_type  # \"single-dict\" or \"multi-dict\"\n",
        "        self.build_path = self.output_path / self.vb_name\n",
        "\n",
        "        # Directory structure\n",
        "        self.dsacoustic = self.build_path / \"dsacoustic\"\n",
        "        self.dsvariance = self.build_path / \"dsvariance\"\n",
        "        self.dsdur = self.build_path / \"dsdur\"\n",
        "        self.dspitch = self.build_path / \"dspitch\"\n",
        "        self.acoustic_embed_dir = self.build_path / \"embeds\" / \"acoustic\"\n",
        "        self.variance_embed_dir = self.build_path / \"embeds\" / \"variance\"\n",
        "        self.acoustic_speakers = []\n",
        "\n",
        "    def _create_directories(self):\n",
        "        for folder in [\n",
        "            self.dsacoustic, self.dsvariance, self.dsdur, self.dspitch,\n",
        "            self.acoustic_embed_dir, self.variance_embed_dir\n",
        "        ]:\n",
        "            folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _process_acoustic_files(self):\n",
        "        print(\"Copying and organizing acoustic files...\")\n",
        "        shutil.copytree(self.acoustic_path, self.build_path, dirs_exist_ok=True)\n",
        "\n",
        "        # Move files to their correct locations\n",
        "        self._move_file(\"acoustic.onnx\", self.dsacoustic)\n",
        "        self._move_files(\"*.emb\", self.acoustic_embed_dir)\n",
        "        self._move_files(\"*.json\", self.dsacoustic)\n",
        "        self._move_files(\"*.txt\", self.dsacoustic)\n",
        "\n",
        "        # Store speaker info for later\n",
        "        self.acoustic_speakers = [\n",
        "            f\"embeds/acoustic/{emb.stem}\" for emb in self.acoustic_embed_dir.glob(\"*.emb\")\n",
        "        ]\n",
        "\n",
        "        # Update and move dsconfig.yaml\n",
        "        dsconfig_path = self.build_path / \"dsconfig.yaml\"\n",
        "        if dsconfig_path.exists():\n",
        "            with dsconfig_path.open(\"r\") as f:\n",
        "                config = yaml.safe_load(f) or {}\n",
        "\n",
        "            # Find language and phoneme files dynamically\n",
        "            try:\n",
        "                lang_file = next(self.dsacoustic.glob(\"*languages.json\"))\n",
        "                config[\"languages\"] = f\"dsacoustic/{lang_file.name}\"\n",
        "            except StopIteration:\n",
        "                print(\"Warning: No languages.json file found for acoustic model.\")\n",
        "\n",
        "            if self.dict_type == \"single-dict\":\n",
        "                try:\n",
        "                    phonemes_file = next(self.dsacoustic.glob(\"*.txt\"))\n",
        "                    new_phoneme_path = self.dsacoustic / \"phonemes.txt\"\n",
        "                    if phonemes_file.name != \"phonemes.txt\":\n",
        "                        phonemes_file.rename(new_phoneme_path)\n",
        "                    config[\"phonemes\"] = \"dsacoustic/phonemes.txt\"\n",
        "                except StopIteration:\n",
        "                    print(\"Warning: No phonemes.txt file found for single-dict acoustic model.\")\n",
        "            else:  # multi-dict\n",
        "                try:\n",
        "                    phonemes_file = next(self.dsacoustic.glob(\"*phonemes.json\"))\n",
        "                    config[\"phonemes\"] = f\"dsacoustic/{phonemes_file.name}\"\n",
        "                except StopIteration:\n",
        "                    print(\"Warning: No phonemes.json file found for acoustic model.\")\n",
        "\n",
        "            config[\"acoustic\"] = \"dsacoustic/acoustic.onnx\"\n",
        "            if self.acoustic_speakers:\n",
        "                config[\"speakers\"] = self.acoustic_speakers\n",
        "\n",
        "            with dsconfig_path.open(\"w\") as f:\n",
        "                yaml.dump(config, f, sort_keys=False)\n",
        "\n",
        "            shutil.move(str(dsconfig_path), self.dsacoustic)\n",
        "        print(\"Done processing acoustic files!\")\n",
        "\n",
        "\n",
        "    def _process_variance_files(self):\n",
        "        print(\"Copying and organizing variance files...\")\n",
        "        shutil.copytree(self.variance_path, self.build_path, dirs_exist_ok=True)\n",
        "\n",
        "        # Move onnx files\n",
        "        self._move_file(\"variance.onnx\", self.dsvariance)\n",
        "        self._move_file(\"pitch.onnx\", self.dspitch)\n",
        "        self._move_file(\"dur.onnx\", self.dsdur)\n",
        "        self._move_file(\"linguistic.onnx\", self.dsacoustic)\n",
        "\n",
        "        # Move embeds\n",
        "        self._move_files(\"*.emb\", self.variance_embed_dir)\n",
        "\n",
        "        # Copy all JSON files (language + phoneme) to variance folders\n",
        "        for json_file in self.build_path.glob(\"*.json\"):\n",
        "            if self.dsvariance.exists(): shutil.copy(json_file, self.dsvariance)\n",
        "            if self.dsdur.exists(): shutil.copy(json_file, self.dsdur)\n",
        "            if self.dspitch.exists(): shutil.copy(json_file, self.dspitch)\n",
        "            json_file.unlink()\n",
        "\n",
        "        # Update and split dsconfig.yaml for variance models\n",
        "        dsconfig_path = self.build_path / \"dsconfig.yaml\"\n",
        "        if dsconfig_path.exists():\n",
        "            with dsconfig_path.open(\"r\") as f:\n",
        "                base_config = yaml.safe_load(f) or {}\n",
        "\n",
        "            # Language file\n",
        "            try:\n",
        "                lang_file = next(self.dsvariance.glob(\"*languages.json\"))\n",
        "                base_config[\"languages\"] = lang_file.name\n",
        "            except StopIteration:\n",
        "                print(\"Warning: No languages.json found in variance model.\")\n",
        "\n",
        "\n",
        "            if self.dict_type == \"single-dict\":\n",
        "                # FORCE phonemes = phonemes.txt in ALL variance-related dsconfig.yaml\n",
        "                print(\"Processing as single-dict  hardcoding phonemes.txt\")\n",
        "                for folder in (self.dsvariance, self.dsdur, self.dspitch):\n",
        "                    if not folder.exists():\n",
        "                        continue\n",
        "                    # Forcing names\n",
        "                    for old in folder.glob(\"*phonemes.*\"):\n",
        "                        new = folder / \"phonemes.txt\"\n",
        "                        if old.name != \"phonemes.txt\":\n",
        "                            print(f\"Renaming {old.name}  phonemes.txt in {folder.name}\")\n",
        "                            old.rename(new)\n",
        "                # phonemes:phonemes.txt\n",
        "                base_config[\"phonemes\"] = \"phonemes.txt\"\n",
        "            else:  # multi-dict\n",
        "                print(\"Processing as multi-dict\")\n",
        "                try:\n",
        "                    phonemes_file = next(self.dsvariance.glob(\"*phonemes.json\"))\n",
        "                    base_config[\"phonemes\"] = phonemes_file.name\n",
        "                except StopIteration:\n",
        "                    print(\"Warning: No phonemes.json file found in variance model.\")\n",
        "\n",
        "            # Common settings\n",
        "            base_config[\"linguistic\"] = \"../dsacoustic/linguistic.onnx\"\n",
        "            variance_speakers = [\n",
        "                f\"../embeds/variance/{emb.stem}\"\n",
        "                for emb in self.variance_embed_dir.glob(\"*.emb\")\n",
        "            ]\n",
        "            if variance_speakers:\n",
        "                base_config[\"speakers\"] = variance_speakers\n",
        "\n",
        "            # Write separate dsconfig for each target folder\n",
        "            targets = {self.dsvariance: \"variance\", self.dsdur: \"dur\", self.dspitch: \"pitch\"}\n",
        "            for folder, key in targets.items():\n",
        "                if not folder.exists(): continue\n",
        "                cfg = base_config.copy()\n",
        "                cfg[key] = f\"{key}.onnx\"\n",
        "                # remove unrelated keys\n",
        "                for k in {\"variance\",\"dur\",\"pitch\"} - {key}:\n",
        "                    cfg.pop(k, None)\n",
        "                with (folder / \"dsconfig.yaml\").open(\"w\") as f:\n",
        "                    yaml.dump(cfg, f, sort_keys=False)\n",
        "            dsconfig_path.unlink()\n",
        "\n",
        "            # bring back acoustic dsconfig and update for single-dict\n",
        "            acoustic_cfg = self.dsacoustic / \"dsconfig.yaml\"\n",
        "            if acoustic_cfg.exists():\n",
        "                shutil.move(str(acoustic_cfg), self.build_path)\n",
        "                # Update root dsconfig for single-dict phonemes path\n",
        "                root_dsconfig = self.build_path / \"dsconfig.yaml\"\n",
        "                if root_dsconfig.exists():\n",
        "                    with root_dsconfig.open(\"r\") as f:\n",
        "                        root_config = yaml.safe_load(f) or {}\n",
        "                    if self.dict_type == \"single-dict\":\n",
        "                        root_config[\"phonemes\"] = \"dsacoustic/phonemes.txt\"\n",
        "                        print(\"Updated root dsconfig phonemes to 'dsacoustic/phonemes.txt'\")\n",
        "                    with root_dsconfig.open(\"w\") as f:\n",
        "                        yaml.dump(root_config, f, sort_keys=False)\n",
        "        print(\"Done processing variance files!\")\n",
        "\n",
        "    def _generate_dictionaries(self):\n",
        "        print(\"Generating dictionary files...\")\n",
        "        txt_files = list(self.build_path.glob(\"*.txt\"))\n",
        "        if not txt_files:\n",
        "            print(\"No dictionary .txt files found. Skipping dictionary generation.\")\n",
        "            return\n",
        "\n",
        "        print(\"Dictionary files found:\", [f.name for f in txt_files])\n",
        "\n",
        "        all_symbols, all_replacements, all_entries = [], [], []\n",
        "        is_single_dict = self.dict_type == \"single-dict\"\n",
        "\n",
        "        for dict_file in txt_files:\n",
        "            parts = dict_file.stem.split(\"-\")\n",
        "            lang_ext = parts[1] if len(parts) > 1 else \"\"\n",
        "\n",
        "            symbols, replacements, entries = [], [], []\n",
        "\n",
        "            with dict_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "                for line in f:\n",
        "                    phoneme_parts = line.strip().split()\n",
        "                    if not phoneme_parts:\n",
        "                        continue\n",
        "                    phoneme = phoneme_parts[0]\n",
        "                    phoneme_type = PHONEME_TYPES.get(phoneme[0], \"stop\")\n",
        "                    full_phoneme = f\"{lang_ext}/{phoneme}\" if not is_single_dict and lang_ext else phoneme\n",
        "\n",
        "                    symbols.append({\"symbol\": full_phoneme, \"type\": phoneme_type})\n",
        "                    replacements.append({\"from\": phoneme, \"to\": full_phoneme})\n",
        "                    entries.append({\"grapheme\": phoneme, \"phonemes\": [full_phoneme]})\n",
        "\n",
        "            all_symbols.extend(symbols)\n",
        "            all_replacements.extend(replacements)\n",
        "            all_entries.extend(entries)\n",
        "\n",
        "            if not is_single_dict and lang_ext:\n",
        "                self._write_dict_yaml(f\"dsdict-{lang_ext}.yaml\", symbols, replacements, entries)\n",
        "\n",
        "        self._write_dict_yaml(\"dsdict.yaml\", all_symbols, all_replacements, all_entries)\n",
        "\n",
        "        # Copy and remove dictionary source/generated files\n",
        "        dest_folders = [d for d in [self.dsvariance, self.dsdur, self.dspitch] if d.exists()]\n",
        "        self._copy_and_remove(\"*.txt\", *dest_folders)\n",
        "        self._copy_and_remove(\"dsdict*.yaml\", *dest_folders)\n",
        "        print(\"Done generating dictionaries!\")\n",
        "\n",
        "    def _write_dict_yaml(self, filename, symbols, replacements, entries):\n",
        "        out_path = self.build_path / filename\n",
        "        print(f\"Writing file: {out_path.name} with {len(symbols)} entries\")\n",
        "\n",
        "        data = {\n",
        "            \"symbols\": [{\"symbol\": \"SP\", \"type\": \"vowel\"}, {\"symbol\": \"AP\", \"type\": \"vowel\"}] + symbols,\n",
        "            \"replacements\": replacements,\n",
        "            \"entries\": entries + [{\"grapheme\": \"SP\", \"phonemes\": [\"SP\"]}, {\"grapheme\": \"AP\", \"phonemes\": [\"AP\"]}]\n",
        "        }\n",
        "        with out_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "            yaml.dump(data, f, sort_keys=False, allow_unicode=True)\n",
        "\n",
        "    def _create_character_files(self):\n",
        "        print(\"Writing character files...\")\n",
        "        # character.txt\n",
        "        with (self.build_path / \"character.txt\").open(\"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(f\"name={self.vb_name}\\nauthor=\\nweb=\\nversion=\\ngroup=\\n\")\n",
        "\n",
        "        # character.yaml\n",
        "        subbanks = []\n",
        "        if self.acoustic_speakers:\n",
        "            for i, spk in enumerate(self.acoustic_speakers, start=1):\n",
        "                speaker_name = Path(spk).name\n",
        "                subbanks.append({\"color\": f\"{i:02d}: {speaker_name}\", \"suffix\": spk})\n",
        "        else:\n",
        "            subbanks.append({\"color\": \"Default\", \"suffix\": \"\"})\n",
        "\n",
        "        char_yaml_data = {\n",
        "            \"portrait_opacity\": 0.5,\n",
        "            \"portrait\": None,\n",
        "            \"image\": None,\n",
        "            \"default_phonemizer\": \"OpenUtau.Core.DiffSinger.DiffSingerPhonemizer\",\n",
        "            \"singer_type\": \"diffsinger\",\n",
        "            \"text_file_encoding\": \"utf-8\",\n",
        "            \"subbanks\": subbanks,\n",
        "        }\n",
        "        with (self.build_path / \"character.yaml\").open(\"w\", encoding=\"utf-8\") as f:\n",
        "            yaml.dump(char_yaml_data, f, sort_keys=False, allow_unicode=True)\n",
        "        print(\"Done writing character files!\")\n",
        "\n",
        "    def _cleanup_and_package(self):\n",
        "        print(\"Cleaning up unnecessary folders...\")\n",
        "        onnx_requirements = {\n",
        "            self.dsvariance: \"variance.onnx\",\n",
        "            self.dsdur: \"dur.onnx\",\n",
        "            self.dspitch: \"pitch.onnx\",\n",
        "        }\n",
        "        for folder, required_file in onnx_requirements.items():\n",
        "            if not (folder / required_file).is_file():\n",
        "                shutil.rmtree(folder, ignore_errors=True)\n",
        "\n",
        "        clear_output()\n",
        "        print(\"Zipping voicebank...\")\n",
        "        zip_path = self.output_path / f\"{self.vb_name}.zip\"\n",
        "        shutil.make_archive(\n",
        "            base_name=str(self.output_path / self.vb_name),\n",
        "            format='zip',\n",
        "            root_dir=self.output_path,\n",
        "            base_dir=self.vb_name\n",
        "        )\n",
        "\n",
        "        print(\"Removing temporary build folder...\")\n",
        "        shutil.rmtree(self.build_path)\n",
        "        print(f\"Voicebank packaged at: {zip_path}\")\n",
        "\n",
        "    def _move_file(self, filename, dest_folder):\n",
        "        src = self.build_path / filename\n",
        "        if src.exists():\n",
        "            print(f\"Moving {src.name} to {dest_folder.relative_to(self.build_path)}\")\n",
        "            shutil.move(str(src), dest_folder)\n",
        "\n",
        "    def _move_files(self, pattern, dest_folder):\n",
        "        for f in self.build_path.glob(pattern):\n",
        "            print(f\"Moving {f.name} to {dest_folder.relative_to(self.build_path)}\")\n",
        "            shutil.move(str(f), dest_folder)\n",
        "\n",
        "    def _copy_and_remove(self, pattern, *dest_folders):\n",
        "        for f in self.build_path.glob(pattern):\n",
        "            if f.name == \"dsconfig.yaml\":\n",
        "                continue\n",
        "            print(f\"Copying {f.name} to variance folders and removing original\")\n",
        "            for folder in dest_folders:\n",
        "                shutil.copy(f, folder)\n",
        "            f.unlink()\n",
        "\n",
        "    def build(self):\n",
        "        if self.build_path.exists():\n",
        "            print(\"Removing existing build directory...\")\n",
        "            shutil.rmtree(self.build_path)\n",
        "\n",
        "        self._create_directories()\n",
        "        self._process_acoustic_files()\n",
        "        self._process_variance_files()\n",
        "        self._generate_dictionaries()\n",
        "        self._create_character_files()\n",
        "        self._cleanup_and_package()\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the voicebank builder.\"\"\"\n",
        "    if not all([acoustic_onnx_folder, variance_onnx_folder, exp_folder, name]):\n",
        "        print(\"Please fill in all the required fields in the form and run this cell again.\")\n",
        "        return\n",
        "\n",
        "    builder = VoicebankBuilder(\n",
        "        acoustic_folder=acoustic_onnx_folder,\n",
        "        variance_folder=variance_onnx_folder,\n",
        "        output_folder=exp_folder,\n",
        "        vb_name=name,\n",
        "        dict_type=dictionary_type,\n",
        "    )\n",
        "    builder.build()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ahNHC1P4CPJ8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Miscellaneous**"
      ],
      "metadata": {
        "id": "4sbU1aH5kGFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #Raw data conversion\n",
        "#@markdown ___\n",
        "%cd /content\n",
        "#@markdown This cell will export .lab and .ds files along with your data\n",
        "\n",
        "data_type = \"lab + wav (NNSVS format)\" # @param [\"lab + wav (NNSVS format)\"]\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> The path to your data zip file\n",
        "\n",
        "data_zip_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> The path you will be saving the data to\n",
        "\n",
        "data_save_path = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "export_ds = True\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> _These values can exceed the amount that's in your data to maximize the segment length or to keep the data as is_\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> Determine how long it will segment your data to based on silence phoneme placement (seconds)\n",
        "segment_length = 15 #@param {type:\"slider\", min:5, max:35, step:1}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> Determine how many silence phoneme is allowed in the middle of each segment\n",
        "max_silence_phoneme_amount = 2 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "\n",
        "# leaving -S at 60 so max silence can be 60 seconds that exceeds the segment legnth cap idk why///\n",
        "# making the segment length cap at 35 secs because any longer than that would make training goes really slow\n",
        "\n",
        "# my ass dont remember why i made two... i think one is unnecessary extra but mehhh\n",
        "all_shits = \"/content/raw_data\"\n",
        "all_shits_not_wav_n_lab = \"/content/raw_data/diffsinger_db\"\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import json\n",
        "import shutil\n",
        "from pydub import AudioSegment\n",
        "\n",
        "if os.path.exists(\"/content/raw_data\"):\n",
        "    shutil.rmtree(\"/content/raw_data\")\n",
        "\n",
        "if not os.path.exists(all_shits_not_wav_n_lab):\n",
        "  os.makedirs(all_shits_not_wav_n_lab)\n",
        "\n",
        "# using 'if not' bc i edited the wrong section which im also too lazy to fix it <3\n",
        "if not data_type == \"lab + wav (NNSVS format)\":\n",
        "    #changed to 7zip to support more compression types\n",
        "    !7z x \"$data_zip_path\" -o{all_shits_not_wav_n_lab}\n",
        "    for root, dirs, files in os.walk(all_shits):\n",
        "        for filename in files:\n",
        "            if filename.endswith(\".lab\"):\n",
        "                file_path = os.path.join(root, filename)\n",
        "                with open(file_path, \"r\") as file:\n",
        "                    file_data = file.read()\n",
        "                file_data = file_data.replace(\"SP\", \"pau\")\n",
        "                file_data = file_data.replace(\"br\", \"AP\")\n",
        "                with open(file_path, \"w\") as file:\n",
        "                    file.write(file_data)\n",
        "\n",
        "else:\n",
        "    !7z x \"$data_zip_path\" -o{all_shits_not_wav_n_lab}\n",
        "\n",
        "\n",
        "# for funny auto dict generator lmao\n",
        "out = \"/content/raw_data/custom_dict.txt\"\n",
        "\n",
        "phonemes = set()\n",
        "\n",
        "def is_excluded(phoneme):\n",
        "    return phoneme in [\"pau\", \"AP\", \"SP\"]\n",
        "\n",
        "if data_type == \"lab + wav (NNSVS format)\":\n",
        "    phoneme_folder_path = all_shits\n",
        "    for root, dirs, files in os.walk(phoneme_folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".lab\"):\n",
        "                fpath = os.path.join(root, file)\n",
        "                with open(fpath, \"r\") as lab_file:\n",
        "                    for line in lab_file:\n",
        "                        line = line.strip()\n",
        "                        if line:\n",
        "                            phoneme = line.split()[2]\n",
        "                            if not is_excluded(phoneme):\n",
        "                                phonemes.add(phoneme)\n",
        "\n",
        "with open(out, \"w\") as f:\n",
        "    for phoneme in sorted(phonemes):\n",
        "        f.write(phoneme + \"\t\" + phoneme + \"\\n\")\n",
        "\n",
        "# for vowels and consonants.txt.... well adding liquid type for uta's script\n",
        "dict_path = out\n",
        "vowel_types = {\"a\", \"i\", \"u\", \"e\", \"o\", \"N\", \"M\", \"NG\"}\n",
        "liquid_types = {\"y\", \"w\", \"l\", \"r\"} # r for english labels, it should be fine with jp too\n",
        "vowel_data = []\n",
        "consonant_data = []\n",
        "liquid_data = []\n",
        "\n",
        "with open(dict_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        phoneme, _ = line.strip().split(\"\\t\")\n",
        "        if phoneme[0] in vowel_types:\n",
        "            vowel_data.append(phoneme)\n",
        "        elif phoneme[0] in liquid_types:\n",
        "            liquid_data.append(phoneme)\n",
        "        else:\n",
        "            consonant_data.append(phoneme)\n",
        "\n",
        "vowel_data.sort()\n",
        "liquid_data.sort()\n",
        "consonant_data.sort()\n",
        "directory = os.path.dirname(dict_path)\n",
        "\n",
        "# make txt for language json file\n",
        "vowel_txt_path = os.path.join(directory, \"vowels.txt\")\n",
        "with open(vowel_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(vowel_data))\n",
        "liquid_txt_path = os.path.join(directory, \"liquids.txt\")\n",
        "with open(liquid_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(liquid_data))\n",
        "consonant_txt_path = os.path.join(directory, \"consonants.txt\")\n",
        "with open(consonant_txt_path, \"w\") as f:\n",
        "    f.write(\" \".join(consonant_data))\n",
        "\n",
        "\n",
        "# here's a funny json append\n",
        "with open(vowel_txt_path, \"r\") as f:\n",
        "    vowel_data = f.read().split()\n",
        "with open(liquid_txt_path, \"r\") as f:\n",
        "    liquid_data = f.read().split()\n",
        "with open(consonant_txt_path, \"r\") as f:\n",
        "    consonant_data = f.read().split()\n",
        "phones4json = {\"vowels\": vowel_data, \"liquids\": liquid_data}\n",
        "with open(\"/content/nnsvs-db-converter/lang.sample.json\", \"w\") as rawr:\n",
        "    json.dump(phones4json, rawr, indent=4)\n",
        "\n",
        "\n",
        "if data_type == \"lab + wav (NNSVS format)\":\n",
        "    db_converter_script = \"/content/nnsvs-db-converter/db_converter.py\"\n",
        "    for raw_folder_name in os.listdir(all_shits_not_wav_n_lab):\n",
        "        raw_folder_path = os.path.join(all_shits_not_wav_n_lab, raw_folder_name)\n",
        "        if os.path.isdir(raw_folder_path):\n",
        "            !python {db_converter_script} -s {max_silence_phoneme_amount} -S 60 -l {segment_length} ${export_lab} -mD -c -L \"/content/nnsvs-db-converter/lang.sample.json\" -w htk --folder {raw_folder_path}\n",
        "\n",
        "if data_type == \"lab + wav (NNSVS format)\":\n",
        "    for raw_folder_name in os.listdir(all_shits_not_wav_n_lab):\n",
        "        raw_folder_path = os.path.join(all_shits_not_wav_n_lab, raw_folder_name)\n",
        "        !rm -rf {raw_folder_path}/*.wav {raw_folder_path}/*.lab\n",
        "        !mv {raw_folder_path}/diffsinger_db/* {raw_folder_path} 2> /dev/null\n",
        "        !rm -rf {raw_folder_path}/diffsinger_db\n",
        "        #!cp {raw_folder_path}/wavs/*.wav {raw_folder_path}\n",
        "\n",
        "# make it replace the first SP to AP cus it seems like people always forgot about it\n",
        "for root, _, files in os.walk(all_shits_not_wav_n_lab):\n",
        "    for file in files:\n",
        "        if file.endswith(\".csv\"):\n",
        "            file_path = os.path.join(root, file)\n",
        "            with open(file_path, \"r\", newline=\"\") as input_file:\n",
        "                csv_reader = csv.reader(input_file)\n",
        "                data = [row for row in csv_reader]\n",
        "                header = data[0]\n",
        "                if \"ph_seq\" in header:\n",
        "                    ph_seq_index = header.index(\"ph_seq\")\n",
        "                    if len(data) > 1 and len(data[1]) > ph_seq_index:\n",
        "                        data[1][ph_seq_index] = data[1][ph_seq_index].replace(\"SP\", \"AP\", 1)\n",
        "            with open(file_path, \"w\", newline=\"\") as output_file:\n",
        "                csv_writer = csv.writer(output_file)\n",
        "                csv_writer.writerows(data)\n",
        "\n",
        "print(\"extraction complete!\")\n",
        "print(\"\\n\")\n",
        "print(\"zipping up files...\")\n",
        "!zip -q -9 -r {data_save_path}/data.zip /content/raw_data/*"
      ],
      "metadata": {
        "cellView": "form",
        "id": "AI7EQ2jQkGEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Build OpenUtau VB (Old Multidict)\n",
        "#@markdown ___\n",
        "# Original script by MLo7Ghinsan: https://github.com/MLo7Ghinsan/ghin_shenanigans/blob/main/scripts/voicebank_exporter.py\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import yaml\n",
        "\n",
        "#@markdown Enter the folders and voicebank name:\n",
        "acoustic_onnx_folder = \"\"  #@param{type:\"string\"}\n",
        "variance_onnx_folder = \"\"  #@param{type:\"string\"}\n",
        "exp_folder = \"\"            #@param{type:\"string\"}\n",
        "name = \"\"                  #@param{type:\"string\"}\n",
        "\n",
        "phoneme_types_list = {\n",
        "    \"a\": \"vowel\", \"i\": \"vowel\", \"u\": \"vowel\", \"e\": \"vowel\", \"o\": \"vowel\",\n",
        "    \"b\": \"stop\", \"d\": \"stop\", \"g\": \"stop\", \"k\": \"stop\", \"p\": \"stop\", \"q\": \"stop\", \"t\": \"stop\",\n",
        "    \"c\": \"affricate\", \"j\": \"affricate\",\n",
        "    \"f\": \"fricative\", \"h\": \"fricative\", \"s\": \"fricative\", \"v\": \"fricative\", \"z\": \"fricative\",\n",
        "    \"l\": \"liquid\", \"r\": \"liquid\",\n",
        "    \"m\": \"nasal\", \"n\": \"nasal\",\n",
        "    \"w\": \"semivowel\", \"y\": \"semivowel\",\n",
        "}\n",
        "\n",
        "\n",
        "def build_ou(acoustic_onnx_folder, variance_onnx_folder, name, output):\n",
        "    ou_build_out = os.path.join(output, name)\n",
        "\n",
        "    dsacoustic = os.path.join(ou_build_out, \"dsacoustic\")\n",
        "    dsvariance = os.path.join(ou_build_out, \"dsvariance\")\n",
        "    dsdur = os.path.join(ou_build_out, \"dsdur\")\n",
        "    dspitch = os.path.join(ou_build_out, \"dspitch\")\n",
        "\n",
        "    acoustic_embed_target_dir = os.path.join(ou_build_out, \"embeds\", \"acoustic\")\n",
        "    variance_embed_target_dir = os.path.join(ou_build_out, \"embeds\", \"variance\")\n",
        "    phonemes_json_target_dir = dsacoustic\n",
        "    lang_dictionary_target_dir = dsacoustic\n",
        "\n",
        "    print(\"Making folder directories\")\n",
        "    for folder in [\n",
        "        dsacoustic, dsvariance, dsdur, dspitch,\n",
        "        acoustic_embed_target_dir,\n",
        "        variance_embed_target_dir\n",
        "    ]:\n",
        "        os.makedirs(folder, exist_ok=True)\n",
        "\n",
        "    print(\"Copying acoustic files\")\n",
        "    shutil.copytree(acoustic_onnx_folder, ou_build_out, dirs_exist_ok=True)\n",
        "\n",
        "    print(\"Moving acoustic items...\")\n",
        "    acoustic_onnx = os.path.join(ou_build_out, \"acoustic.onnx\")\n",
        "    if os.path.exists(acoustic_onnx):\n",
        "        print(f\"Found: {os.path.basename(acoustic_onnx)}, moving it to dsacoustic\")\n",
        "        shutil.move(acoustic_onnx, dsacoustic)\n",
        "\n",
        "    embeds_files = glob.glob(os.path.join(ou_build_out, \"*.emb\"))\n",
        "    for embeds_file in embeds_files:\n",
        "        if os.path.exists(embeds_file):\n",
        "            print(f\"Found: {os.path.basename(embeds_file)}, moving it to {os.path.join('embeds', 'acoustic')}\")\n",
        "            shutil.move(embeds_file, acoustic_embed_target_dir)\n",
        "\n",
        "    phonemes_json_files = glob.glob(os.path.join(ou_build_out, \"*.json\"))\n",
        "    for phonemes_json_file in phonemes_json_files:\n",
        "        if os.path.exists(phonemes_json_file):\n",
        "            print(f\"Found: {os.path.basename(phonemes_json_file)}, moving it to dsacoustic\")\n",
        "            shutil.move(phonemes_json_file, phonemes_json_target_dir)\n",
        "\n",
        "    lang_dictionary_files = glob.glob(os.path.join(ou_build_out, \"*.txt\"))\n",
        "    for lang_dictionary_file in lang_dictionary_files:\n",
        "        if os.path.exists(lang_dictionary_file):\n",
        "            print(f\"Found: {os.path.basename(lang_dictionary_file)}, moving it to dsacoustic\")\n",
        "            shutil.move(lang_dictionary_file, lang_dictionary_target_dir)\n",
        "\n",
        "    # ill use this later in character.yaml\n",
        "    acoustic_speakers = [\n",
        "            os.path.join(\"embeds\", \"acoustic\", os.path.splitext(os.path.basename(emb))[0])\n",
        "            for emb in embeds_files\n",
        "    ]\n",
        "\n",
        "    print(\"Appending dsconfig.yaml for acoustic\")\n",
        "    dsconfig_path = os.path.join(ou_build_out, \"dsconfig.yaml\")\n",
        "    if os.path.exists(dsconfig_path):\n",
        "        with open(dsconfig_path, \"r\") as config:\n",
        "            acoustic_config = yaml.safe_load(config)\n",
        "\n",
        "        acoustic_config[\"acoustic\"] = os.path.join(\"dsacoustic\", \"acoustic.onnx\")\n",
        "        acoustic_config[\"languages\"] = os.path.join(\"dsacoustic\", \"acoustic_languages.json\")\n",
        "        acoustic_config[\"phonemes\"] = os.path.join(\"dsacoustic\", \"acoustic_phonemes.json\")\n",
        "        acoustic_config[\"speakers\"] = acoustic_speakers\n",
        "\n",
        "        with open(dsconfig_path, \"w\") as config:\n",
        "            yaml.dump(acoustic_config, config)\n",
        "\n",
        "        # Temporarily move dsconfig.yaml into dsacoustic\n",
        "        shutil.move(dsconfig_path, dsacoustic)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    print(\"Copying variance files\")\n",
        "    shutil.copytree(variance_onnx_folder, ou_build_out, dirs_exist_ok=True)\n",
        "\n",
        "    print(\"Moving variance items\")\n",
        "    variance_files = [\n",
        "        (\"variance.onnx\", \"dsvariance\"),\n",
        "        (\"pitch.onnx\", \"dspitch\"),\n",
        "        (\"dur.onnx\", \"dsdur\"),\n",
        "        (\"linguistic.onnx\", \"dsacoustic\"),\n",
        "    ]\n",
        "\n",
        "    for filename, variance_folder in variance_files:\n",
        "        source = os.path.join(ou_build_out, filename)\n",
        "        destination = os.path.join(ou_build_out, variance_folder)\n",
        "        if os.path.exists(source):\n",
        "            print(f\"Found: {os.path.basename(source)}, moving it to {destination}\")\n",
        "            shutil.move(source, destination)\n",
        "\n",
        "    embeds_files = glob.glob(os.path.join(ou_build_out, \"*.emb\"))\n",
        "    for embeds_file in embeds_files:\n",
        "        if os.path.exists(embeds_file):\n",
        "            print(f\"Found: {os.path.basename(embeds_file)}, moving it to {os.path.join('embeds', 'variance')}\")\n",
        "            shutil.move(embeds_file, variance_embed_target_dir)\n",
        "\n",
        "    phonemes_json_files = glob.glob(os.path.join(ou_build_out, \"*.json\"))\n",
        "    for phonemes_json_file in phonemes_json_files:\n",
        "        if os.path.exists(phonemes_json_file):\n",
        "            print(f\"Found: {os.path.basename(phonemes_json_file)}, copying it to dsvariance, dsdur, and dspitch\")\n",
        "            shutil.copy(phonemes_json_file, dsvariance)\n",
        "            shutil.copy(phonemes_json_file, dsdur)\n",
        "            shutil.copy(phonemes_json_file, dspitch)\n",
        "            os.remove(phonemes_json_file)\n",
        "\n",
        "    lang_dictionary_files = glob.glob(os.path.join(ou_build_out, \"*.txt\"))\n",
        "    print(\"Dictionary files found:\", [os.path.basename(dict_name) for dict_name in lang_dictionary_files])\n",
        "\n",
        "    all_symbols = []\n",
        "    all_replacements = []\n",
        "    all_entries = []\n",
        "\n",
        "    for dictionary_file in lang_dictionary_files:\n",
        "        dictionary_ext = os.path.basename(dictionary_file).split(\"-\")[1].split(\".\")[0]\n",
        "\n",
        "        symbols = []\n",
        "        replacements = []\n",
        "        entries = []\n",
        "\n",
        "        with open(dictionary_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                phoneme = line.strip().split()[0]\n",
        "                phoneme_type = phoneme_types_list.get(phoneme[0], \"stop\")\n",
        "                symbol_entry = {\"symbol\": f\"{dictionary_ext}/{phoneme}\", \"type\": phoneme_type}\n",
        "                replacement_entry = {\"from\": phoneme, \"to\": f\"{dictionary_ext}/{phoneme}\"}\n",
        "                entry_entry = {\"grapheme\": phoneme, \"phonemes\": [f\"{dictionary_ext}/{phoneme}\"]}\n",
        "\n",
        "                symbols.append(symbol_entry)\n",
        "                replacements.append(replacement_entry)\n",
        "                entries.append(entry_entry)\n",
        "\n",
        "                all_symbols.append(symbol_entry)\n",
        "                all_replacements.append(replacement_entry)\n",
        "                all_entries.append(entry_entry)\n",
        "\n",
        "        out_path = os.path.join(ou_build_out, f\"dsdict-{dictionary_ext}.yaml\")\n",
        "        print(f\"Writing file: dsdict-{dictionary_ext}.yaml with {len(symbols)} entries\")\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "            out_f.write(\"symbols:\\n\")\n",
        "            out_f.write('- {symbol: SP, type: vowel}\\n')\n",
        "            out_f.write('- {symbol: AP, type: vowel}\\n')\n",
        "            for item in symbols:\n",
        "                out_f.write(f\"- {{symbol: {item['symbol']}, type: {item['type']}}}\\n\")\n",
        "            out_f.write(\"\\nreplacements:\\n\")\n",
        "            for item in replacements:\n",
        "                out_f.write(f\"- {{from: \\\"{item['from']}\\\", to: \\\"{item['to']}\\\"}}\\n\")\n",
        "            out_f.write(\"\\nentries:\\n\")\n",
        "            for item in entries:\n",
        "                out_f.write(f\"- {{grapheme: \\\"{item['grapheme']}\\\", phonemes: [\\\"{item['phonemes'][0]}\\\"]}}\\n\")\n",
        "            out_f.write('- {grapheme: \"SP\", phonemes: [SP]}\\n')\n",
        "            out_f.write('- {grapheme: \"AP\", phonemes: [AP]}\\n')\n",
        "\n",
        "    dsdict_path = os.path.join(ou_build_out, \"dsdict.yaml\")\n",
        "    print(f\"Writing file: dsdict.yaml with {len(all_symbols)} entries\")\n",
        "    with open(dsdict_path, \"w\", encoding=\"utf-8\") as out_f:\n",
        "        out_f.write(\"symbols:\\n\")\n",
        "        out_f.write('- {symbol: SP, type: vowel}\\n')\n",
        "        out_f.write('- {symbol: AP, type: vowel}\\n')\n",
        "        for item in all_symbols:\n",
        "            out_f.write(f\"- {{symbol: {item['symbol']}, type: {item['type']}}}\\n\")\n",
        "        out_f.write(\"\\nreplacements:\\n\")\n",
        "        for item in all_replacements:\n",
        "            out_f.write(f\"- {{from: \\\"{item['from']}\\\", to: \\\"{item['to']}\\\"}}\\n\")\n",
        "        out_f.write(\"\\nentries:\\n\")\n",
        "        for item in all_entries:\n",
        "            out_f.write(f\"- {{grapheme: \\\"{item['grapheme']}\\\", phonemes: [\\\"{item['phonemes'][0]}\\\"]}}\\n\")\n",
        "        out_f.write('- {grapheme: \"SP\", phonemes: [SP]}\\n')\n",
        "        out_f.write('- {grapheme: \"AP\", phonemes: [AP]}\\n')\n",
        "\n",
        "    print(f\"Copying generated dictionary to dsvariance, dsdur, and dspitch\")\n",
        "\n",
        "    lang_dictionary_files = glob.glob(os.path.join(ou_build_out, \"*.txt\"))\n",
        "    for lang_dictionary_file in lang_dictionary_files:\n",
        "        if os.path.exists(lang_dictionary_file):\n",
        "            shutil.copy(lang_dictionary_file, dsvariance)\n",
        "            shutil.copy(lang_dictionary_file, dsdur)\n",
        "            shutil.copy(lang_dictionary_file, dspitch)\n",
        "            os.remove(lang_dictionary_file)\n",
        "\n",
        "    lang_dictionary_files_yaml = glob.glob(os.path.join(ou_build_out, \"*.yaml\"))\n",
        "    for yaml_dictionary_file in lang_dictionary_files_yaml:\n",
        "        filename = os.path.basename(yaml_dictionary_file)\n",
        "        if filename != \"dsconfig.yaml\" and os.path.exists(yaml_dictionary_file):\n",
        "            shutil.copy(yaml_dictionary_file, dsvariance)\n",
        "            shutil.copy(yaml_dictionary_file, dsdur)\n",
        "            shutil.copy(yaml_dictionary_file, dspitch)\n",
        "            os.remove(yaml_dictionary_file)\n",
        "\n",
        "    print(\"Appending dsconfig.yaml for variance\")\n",
        "    dsconfig_path = os.path.join(ou_build_out, \"dsconfig.yaml\")\n",
        "    if os.path.exists(dsconfig_path):\n",
        "        with open(dsconfig_path, \"r\") as config:\n",
        "            base_config = yaml.safe_load(config)\n",
        "\n",
        "        base_config[\"languages\"] = \"variance_languages.json\"\n",
        "        base_config[\"phonemes\"] = \"variance_phonemes.json\"\n",
        "        base_config[\"linguistic\"] = os.path.join(\"..\", \"dsacoustic\", \"linguistic.onnx\")\n",
        "        base_config[\"speakers\"] = [\n",
        "            os.path.join(\"..\", \"embeds\", \"variance\", os.path.splitext(os.path.basename(emb))[0])\n",
        "            for emb in embeds_files\n",
        "        ]\n",
        "\n",
        "        target_configs = {\n",
        "            dsvariance: \"variance\",\n",
        "            dsdur: \"dur\",\n",
        "            dspitch: \"pitch\"\n",
        "        }\n",
        "\n",
        "        for folder, key_to_add in target_configs.items():\n",
        "            config_copy = base_config.copy()\n",
        "            config_copy[key_to_add] = f\"{key_to_add}.onnx\"\n",
        "\n",
        "            for key in {\"variance\", \"dur\", \"pitch\"} - {key_to_add}:\n",
        "                config_copy.pop(key, None)\n",
        "\n",
        "            output_config_path = os.path.join(folder, \"dsconfig.yaml\")\n",
        "            with open(output_config_path, \"w\") as f:\n",
        "                yaml.dump(config_copy, f,)\n",
        "    os.remove(dsconfig_path)\n",
        "    shutil.move(os.path.join(dsacoustic, \"dsconfig.yaml\"), ou_build_out)\n",
        "    print(\"Done!\")\n",
        "\n",
        "    print(\"Cleaning folders\")\n",
        "    onnx_requirements = {\n",
        "        dsvariance: \"variance.onnx\",\n",
        "        dsdur: \"dur.onnx\",\n",
        "        dspitch: \"pitch.onnx\",\n",
        "    }\n",
        "\n",
        "    for folder, required_file in onnx_requirements.items():\n",
        "        required_path = os.path.join(folder, required_file)\n",
        "        if not os.path.isfile(required_path):\n",
        "            shutil.rmtree(folder)\n",
        "\n",
        "    print(\"Writing file: character.txt\")\n",
        "    lines = [\n",
        "        f\"name={name}\\n\",\n",
        "        \"author=\\n\",\n",
        "        \"web=\\n\",\n",
        "        \"version=\\n\",\n",
        "        \"group=\\n\"\n",
        "    ]\n",
        "    with open(os.path.join(ou_build_out, \"character.txt\"), \"w\", encoding=\"utf-8\") as file:\n",
        "        file.writelines(lines)\n",
        "\n",
        "\n",
        "    print(\"Writing file: character.yaml\")\n",
        "    lines = [\n",
        "        \"portrait_opacity: 0.5\\n\",\n",
        "        \"portrait:\\n\",\n",
        "        \"image:\\n\",\n",
        "        \"default_phonemizer: OpenUtau.Core.DiffSinger.DiffSingerPhonemizer\\n\",\n",
        "        \"singer_type: diffsinger\\n\",\n",
        "        \"text_file_encoding: utf-8\\n\",\n",
        "        \"subbanks:\\n\"\n",
        "    ]\n",
        "\n",
        "    for i, spk in enumerate(acoustic_speakers, start=1):\n",
        "        speaker_name = os.path.basename(spk)\n",
        "        lines.append(f\" - color: \\\"{i:02d}: {speaker_name}\\\"\\n\")\n",
        "        lines.append(f\"   suffix: {spk}\\n\")\n",
        "    with open(os.path.join(ou_build_out, \"character.yaml\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.writelines(lines)\n",
        "\n",
        "    clear_output()\n",
        "    print(\"Zipping...\")\n",
        "    zip_path = os.path.join(output, f\"{name}.zip\")\n",
        "    shutil.make_archive(base_name=os.path.splitext(zip_path)[0], format='zip', root_dir=output, base_dir=name)\n",
        "\n",
        "    print(\"Removing temporary folder...\")\n",
        "    shutil.rmtree(os.path.join(output, name))\n",
        "\n",
        "    print(f\"Voicebank packaged at: {zip_path}\")\n",
        "\n",
        "\n",
        "build_ou(acoustic_onnx_folder, variance_onnx_folder, name=name, output=exp_folder)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "wpvtmnwkshIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Build OpenUtau VB\n",
        "#@markdown ___\n",
        "#i need to clean this up it seems\n",
        "#plan: add a build ou section here by inserting onnx paths (or just the folder containing the folders to the onnx files) to build ou\n",
        "# ill have a config read function too so i dont have to add checkmark of if people train with embeds or shallow diff or not <3\n",
        "# yes im lazy rawr x3\n",
        "%cd /content\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from IPython.display import clear_output\n",
        "\n",
        "constr_folder = \"/content/OU_voicebank\"\n",
        "if not os.path.exists(constr_folder):\n",
        "    os.makedirs(constr_folder)\n",
        "else:\n",
        "    shutil.rmtree(constr_folder)\n",
        "    os.makedirs(constr_folder)\n",
        "\n",
        "clear_output()\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> path to your **ACOUSTIC ONNX FOLDER**\n",
        "acoustic_onnx_folder = \"\" #@param{type:\"string'}\n",
        "#@markdown <font size=\"-1.5\"> path to the config.yaml of acoustic model\n",
        "acoustic_config = \"\" #@param{type:\"string'}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> path to your **VARIANCE ONNX FOLDER**\n",
        "variance_onnx_folder = \"\" #@param{type:\"string'}\n",
        "#@markdown <font size=\"-1.5\"> path to the config.yaml of variance model\n",
        "variance_config = \"\" #@param{type:\"string'}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> path to your word to phoneme dict (leave blank to use default Japanese dict)\n",
        "dictionary_path = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> path to the folder you want to save the zip file to\n",
        "save_path = \"\" #@param{type:\"string\"}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown ## Character Configuration | character.txt and character.yaml\n",
        "\n",
        "#@markdown <font size=\"-1.5\"> your character display name| **required**\n",
        "name = \"\" #@param{type:\"string\"}\n",
        "\n",
        "print(\"copying files...\")\n",
        "main_stuff = f\"{constr_folder}/{name}\"\n",
        "if not os.path.exists(main_stuff):\n",
        "    os.makedirs(main_stuff)\n",
        "if not os.path.exists(f\"{main_stuff}/dsmain\"):\n",
        "    os.makedirs(f\"{main_stuff}/dsmain/embeds/acoustic\")\n",
        "    os.makedirs(f\"{main_stuff}/dsmain/embeds/variance\")\n",
        "!cp {acoustic_onnx_folder}/acoustic.onnx {main_stuff}/dsmain\n",
        "!cp {acoustic_onnx_folder}/phonemes.txt {main_stuff}/dsmain\n",
        "!cp {acoustic_onnx_folder}/*.emb {main_stuff}/dsmain/embeds/acoustic >/dev/null 2>&1\n",
        "!cp {variance_onnx_folder}/*.emb {main_stuff}/dsmain/embeds/variance >/dev/null 2>&1\n",
        "\n",
        "if variance_onnx_folder:\n",
        "    !cp {variance_onnx_folder}/linguistic.onnx {main_stuff}/dsmain\n",
        "else:\n",
        "    pass\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"writing character.txt...\")\n",
        "with open(f\"{main_stuff}/character.txt\", \"w\") as file:\n",
        "    file.write(f\"name={name}\\n\")\n",
        "    file.write(\"image=\\n\")\n",
        "    file.write(\"author=\\n\")\n",
        "    file.write(\"voice=\\n\")\n",
        "    file.write(\"web=\\n\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"writing character.yaml...\")\n",
        "with open(f\"{main_stuff}/character.yaml\", \"w\") as file:\n",
        "    file.write(\"text_file_encoding: utf-8\\n\")\n",
        "    file.write(\"portrait:\\n\")\n",
        "    file.write(\"portrait_opacity: 0.45\\n\")\n",
        "    file.write(\"default_phonemizer: OpenUtau.Core.DiffSinger.DiffSingerPhonemizer\\n\")\n",
        "    file.write(\"singer_type: diffsinger\\n\")\n",
        "acoustic_emb_files = os.listdir(acoustic_onnx_folder)\n",
        "acoustic_embeds = []\n",
        "acoustic_color_suffix = []\n",
        "for file in acoustic_emb_files:\n",
        "    if file.endswith(\".emb\"):\n",
        "        acoustic_emb = os.path.splitext(file)[0]\n",
        "        acoustic_embeds.append(\"dsmain/embeds/acoustic/\" + acoustic_emb)\n",
        "        acoustic_color_suffix.append(acoustic_emb)\n",
        "subbanks = []\n",
        "for i, (acoustic_embed_color, acoustic_embed_suffix) in enumerate(zip(acoustic_color_suffix, acoustic_embeds), start=1):\n",
        "    color = f\"{i:02}: {acoustic_embed_color}\"\n",
        "    suffix = f\"{acoustic_embed_suffix}\"\n",
        "    subbanks.append({\"color\": color, \"suffix\": suffix})\n",
        "if subbanks:\n",
        "    with open(f\"{main_stuff}/character.yaml\", \"r\") as config:\n",
        "        i_wanna_die_slash_j = yaml.safe_load(config)\n",
        "    i_wanna_die_slash_j[\"subbanks\"] = subbanks\n",
        "    with open(f\"{main_stuff}/character.yaml\", \"w\") as config:\n",
        "        yaml.dump(i_wanna_die_slash_j, config)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"writing dsconfig.yaml for acoustic...\")\n",
        "with open(f\"{main_stuff}/dsconfig.yaml\", \"w\") as file:\n",
        "    file.write(\"phonemes: dsmain/phonemes.txt\\n\")\n",
        "    file.write(\"acoustic: dsmain/acoustic.onnx\\n\")\n",
        "    file.write(\"vocoder: nsf_hifigan\\n\")\n",
        "    file.write(\"singer_type: diffsinger\\n\")\n",
        "with open(acoustic_config, \"r\") as config:\n",
        "    mfking_config = yaml.safe_load(config)\n",
        "use_energy_embed = mfking_config.get(\"use_energy_embed\")\n",
        "use_breathiness_embed = mfking_config.get(\"use_breathiness_embed\")\n",
        "use_shallow_diffusion = mfking_config.get(\"use_shallow_diffusion\")\n",
        "max_depth = mfking_config.get(\"T_start\")\n",
        "speakers = mfking_config.get(\"speakers\") #looking back here, why is this even here lmao cus i used acoustic_embeds instead of speakers\n",
        "augmentation_arg = mfking_config.get(\"augmentation_args\")\n",
        "pitch_aug = mfking_config.get(\"use_key_shift_embed\")\n",
        "time_aug = mfking_config.get(\"use_speed_embed\")\n",
        "voicing = mfking_config.get(\"use_voicing_embed\")\n",
        "tension = mfking_config.get(\"use_tension_embed\")\n",
        "sample_rate = mfking_config.get(\"audio_sample_rate\")\n",
        "hop_size = mfking_config.get(\"hop_size\")\n",
        "win_size = mfking_config.get(\"win_size\")\n",
        "fft_size = mfking_config.get(\"fft_size\")\n",
        "num_mel_bins = mfking_config.get(\"audio_num_mel_bins\")\n",
        "mel_fmin = mfking_config.get(\"fmin\")\n",
        "mel_fmax = mfking_config.get(\"fmax\")\n",
        "mel_base = mfking_config.get(\"mel_base\")\n",
        "\n",
        "with open(f\"{main_stuff}/dsconfig.yaml\", \"r\") as config:\n",
        "    why_are_there_so_many_i_could_prob_make_it_one = yaml.safe_load(config)\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_energy_embed\"] = use_energy_embed\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_breathiness_embed\"] = use_breathiness_embed\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_variable_depth\"] = use_shallow_diffusion\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"max_depth\"] = max_depth\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"augmentation_args\"] = augmentation_arg\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_key_shift_embed\"] = pitch_aug\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_speed_embed\"] = time_aug\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_voicing_embed\"] = voicing\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_tension_embed\"] = tension\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"use_continuous_acceleration\"] = True\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"sample_rate\"] = sample_rate\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"hop_size\"] = hop_size\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"win_size\"] = win_size\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"fft_size\"] = fft_size\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"num_mel_bins\"] = num_mel_bins\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"fmin\"] = mel_fmin\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"fmax\"] = mel_fmax\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"mel_base\"] = mel_base\n",
        "why_are_there_so_many_i_could_prob_make_it_one[\"mel_scale\"] = \"slaney\"\n",
        "\n",
        "\n",
        "if subbanks:\n",
        "    why_are_there_so_many_i_could_prob_make_it_one[\"speakers\"] = acoustic_embeds\n",
        "with open(f\"{main_stuff}/dsconfig.yaml\", \"w\") as config:\n",
        "    yaml.dump(why_are_there_so_many_i_could_prob_make_it_one, config)\n",
        "\n",
        "\n",
        "variance_emb_files = os.listdir(variance_onnx_folder)\n",
        "variance_embeds = []\n",
        "for file in variance_emb_files:\n",
        "    if file.endswith(\".emb\"):\n",
        "        variance_emb = os.path.splitext(file)[0]\n",
        "        variance_embeds.append(\"../dsmain/embeds/variance/\" + variance_emb)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"writing dsdict.yaml...\")\n",
        "if not dictionary_path:\n",
        "    dict_path = \"/content/jpn_dict.txt\"\n",
        "else:\n",
        "    dict_path = dictionary_path\n",
        "\n",
        "# for symbols list\n",
        "phoneme_dict_path = f\"{acoustic_onnx_folder}/dictionary.txt\"\n",
        "\n",
        "dsdict = \"dsdict.yaml\"\n",
        "\n",
        "def parse_phonemes(phonemes_str):\n",
        "    return phonemes_str.split()\n",
        "\n",
        "entries = []\n",
        "vowel_types = {\"a\", \"i\", \"u\", \"e\", \"o\", \"N\", \"M\", \"NG\", \"cl\", \"vf\"}\n",
        "vowel_data = []\n",
        "stop_data = []\n",
        "\n",
        "# Process the specified dictionary\n",
        "with open(dict_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        word, phonemes_str = line.strip().split(\"\\t\")\n",
        "        phonemes = parse_phonemes(phonemes_str)\n",
        "        if len(phonemes) == 1:\n",
        "            entries.append({\"grapheme\": word, \"phonemes\": phonemes})\n",
        "        else:\n",
        "            entries.append({\"grapheme\": word, \"phonemes\": phonemes})\n",
        "\n",
        "with open(phoneme_dict_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        phoneme, _ = line.strip().split(\"\\t\")\n",
        "        phoneme_type = \"vowel\" if phoneme[0] in vowel_types else \"stop\"\n",
        "        entry = {\"symbol\": phoneme, \"type\": phoneme_type}\n",
        "        if phoneme_type == \"vowel\":\n",
        "            vowel_data.append(entry)\n",
        "        else:\n",
        "            stop_data.append(entry)\n",
        "\n",
        "vowel_data.sort(key=lambda x: x[\"symbol\"])\n",
        "stop_data.sort(key=lambda x: x[\"symbol\"])\n",
        "\n",
        "dsdict_path = os.path.join(constr_folder, dsdict)\n",
        "with open(dsdict_path, \"w\") as f:\n",
        "    f.write(\"entries:\\n\")\n",
        "    for entry in entries:\n",
        "        f.write(f\"- grapheme: {entry['grapheme']}\\n\")\n",
        "        f.write(\"  phonemes:\\n\")\n",
        "        for phoneme in entry[\"phonemes\"]:\n",
        "            f.write(f\"  - {phoneme}\\n\")\n",
        "\n",
        "    f.write(\"\\nsymbols:\\n\")\n",
        "    for entry in vowel_data + stop_data:\n",
        "        f.write(f\"- symbol: {entry['symbol']}\\n\")\n",
        "        f.write(f\"  type: {entry['type']}\\n\")\n",
        "\n",
        "with open(variance_config, \"r\") as config:\n",
        "    mfking_config = yaml.safe_load(config)\n",
        "sample_rate = mfking_config.get(\"audio_sample_rate\")\n",
        "hop_size = mfking_config.get(\"hop_size\")\n",
        "predict_dur = mfking_config.get(\"predict_dur\")\n",
        "predict_pitch = mfking_config.get(\"predict_pitch\")\n",
        "use_melody_encoder = mfking_config.get(\"use_melody_encoder\")\n",
        "predict_voicing = mfking_config.get(\"predict_voicing\")\n",
        "predict_tension = mfking_config.get(\"predict_tension\")\n",
        "predict_energy = mfking_config.get(\"predict_energy\")\n",
        "predict_breathiness = mfking_config.get(\"predict_breathiness\")\n",
        "\n",
        "dur_onnx_path = variance_onnx_folder + \"/dur.onnx\"\n",
        "if os.path.exists(dur_onnx_path):\n",
        "    print(\"\\n\")\n",
        "    print(\"making dsdur directory and necessary files...\")\n",
        "    os.makedirs(f\"{main_stuff}/dsdur\")\n",
        "    !cp {dur_onnx_path} {main_stuff}/dsdur\n",
        "    !cp {dsdict_path} {main_stuff}/dsdur\n",
        "    with open(f\"{main_stuff}/dsdur/dsconfig.yaml\", \"w\") as file:\n",
        "        file.write(\"phonemes: ../dsmain/phonemes.txt\\n\")\n",
        "        file.write(\"linguistic: ../dsmain/linguistic.onnx\\n\")\n",
        "        file.write(\"dur: dur.onnx\\n\")\n",
        "    with open(f\"{main_stuff}/dsdur/dsconfig.yaml\", \"r\") as config:\n",
        "        dsdur_config = yaml.safe_load(config)\n",
        "    dsdur_config[\"use_continuous_acceleration\"] = True\n",
        "    dsdur_config[\"sample_rate\"] = sample_rate\n",
        "    dsdur_config[\"hop_size\"] = hop_size\n",
        "    dsdur_config[\"predict_dur\"] = predict_dur\n",
        "    if subbanks:\n",
        "        dsdur_config[\"speakers\"] = variance_embeds\n",
        "    with open(f\"{main_stuff}/dsdur/dsconfig.yaml\", \"w\") as config:\n",
        "        yaml.dump(dsdur_config, config)\n",
        "else:\n",
        "    print(\"\\n\")\n",
        "    print(\"dur.onnx not found, skipping on making dsdur folder...\")\n",
        "\n",
        "pitch_onnx_path = variance_onnx_folder + \"/pitch.onnx\"\n",
        "if os.path.exists(pitch_onnx_path):\n",
        "    print(\"\\n\")\n",
        "    print(\"making dspitch directory and necessary files...\")\n",
        "    os.makedirs(f\"{main_stuff}/dspitch\")\n",
        "    !cp {pitch_onnx_path} {main_stuff}/dspitch\n",
        "    !cp {dsdict_path} {main_stuff}/dspitch\n",
        "    with open(f\"{main_stuff}/dspitch/dsconfig.yaml\", \"w\") as file:\n",
        "        file.write(\"phonemes: ../dsmain/phonemes.txt\\n\")\n",
        "        file.write(\"linguistic: ../dsmain/linguistic.onnx\\n\")\n",
        "        file.write(\"pitch: pitch.onnx\\n\")\n",
        "        file.write(\"use_expr: true\\n\")\n",
        "    with open(f\"{main_stuff}/dspitch/dsconfig.yaml\", \"r\") as config:\n",
        "        dspitch_config = yaml.safe_load(config)\n",
        "    dspitch_config[\"use_continuous_acceleration\"] = True\n",
        "    dspitch_config[\"sample_rate\"] = sample_rate\n",
        "    dspitch_config[\"hop_size\"] = hop_size\n",
        "    dspitch_config[\"predict_dur\"] = predict_pitch\n",
        "    if subbanks:\n",
        "        dspitch_config[\"speakers\"] = variance_embeds\n",
        "    dspitch_config[\"use_note_rest\"] = use_melody_encoder\n",
        "    with open(f\"{main_stuff}/dspitch/dsconfig.yaml\", \"w\") as config:\n",
        "        yaml.dump(dspitch_config, config)\n",
        "else:\n",
        "    print(\"\\n\")\n",
        "    print(\"pitch.onnx not found, skipping on making dspitch folder...\")\n",
        "\n",
        "variance_onnx_path = variance_onnx_folder + \"/variance.onnx\"\n",
        "if os.path.exists(variance_onnx_path):\n",
        "    print(\"\\n\")\n",
        "    print(\"making dsvariance directory and necessary files...\")\n",
        "    os.makedirs(f\"{main_stuff}/dsvariance\")\n",
        "    !cp {variance_onnx_path} {main_stuff}/dsvariance\n",
        "    !cp {dsdict_path} {main_stuff}/dsvariance\n",
        "    with open(f\"{main_stuff}/dsvariance/dsconfig.yaml\", \"w\") as file:\n",
        "        file.write(\"phonemes: ../dsmain/phonemes.txt\\n\")\n",
        "        file.write(\"linguistic: ../dsmain/linguistic.onnx\\n\")\n",
        "        file.write(\"variance: variance.onnx\\n\")\n",
        "    with open(f\"{main_stuff}/dsvariance/dsconfig.yaml\", \"r\") as config:\n",
        "        dsvariance_config = yaml.safe_load(config)\n",
        "    dsvariance_config[\"use_continuous_acceleration\"] = True\n",
        "    dsvariance_config[\"sample_rate\"] = sample_rate\n",
        "    dsvariance_config[\"hop_size\"] = hop_size\n",
        "    dsvariance_config[\"predict_dur\"] = True #this one will always be true cus if there's no variance model, it shouldnt make this folder in the first place\n",
        "    dsvariance_config[\"predict_voicing\"] = predict_voicing\n",
        "    dsvariance_config[\"predict_tension\"] = predict_tension\n",
        "    dsvariance_config[\"predict_energy\"] = predict_energy\n",
        "    dsvariance_config[\"predict_breathiness\"] = predict_breathiness\n",
        "    if subbanks:\n",
        "        dsvariance_config[\"speakers\"] = variance_embeds\n",
        "    with open(f\"{main_stuff}/dsvariance/dsconfig.yaml\", \"w\") as config:\n",
        "        yaml.dump(dsvariance_config, config)\n",
        "else:\n",
        "    print(\"\\n\")\n",
        "    print(\"variance.onnx not found, skipping on making dsvariance folder...\")\n",
        "\n",
        "!rm -rf {dsdict_path}\n",
        "#im too lazy to write codes so ill just do this, itll only remove those folders if they're empty anyway\n",
        "!rm -d {main_stuff}/dsmain/embeds/* >/dev/null 2>&1\n",
        "!rm -d {main_stuff}/dsmain/embeds >/dev/null 2>&1\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"zipping up files...\")\n",
        "!zip -q -9 -r {save_path}/{name}.zip {main_stuff}/*\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"done!\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"You can download your model zip and use it in OpenUtau! If anything needed to be edit in the config then please do so\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "A70Sc3Hbmxh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}