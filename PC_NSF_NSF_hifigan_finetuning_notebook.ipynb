{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/usamireko/DiffSinger_colab_notebook_MLo7/blob/Stable/PC_NSF_NSF_hifigan_finetuning_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "6jPo7zvZQ6GA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zW_ptHQW9FXh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Mount Google Drive and Setup with Python 3.10\n",
        "%cd /content\n",
        "pc_nsf = True  # @param {\"type\":\"boolean\"}\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import os\n",
        "import shutil\n",
        "import yaml\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Install Python 3.10\n",
        "!sudo apt update -y\n",
        "!sudo apt install -y python3.10 python3.10-distutils python3.10-venv\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1\n",
        "!sudo update-alternatives --set python3 /usr/bin/python3.10\n",
        "!curl -sS https://bootstrap.pypa.io/get-pip.py | python3\n",
        "\n",
        "# Install UV\n",
        "!python3 -m pip install -U uv\n",
        "\n",
        "# Add UV to PATH\n",
        "os.environ[\"PATH\"] += \":/root/.cargo/bin\"\n",
        "\n",
        "# Verify versions\n",
        "!python3 --version\n",
        "!uv --version\n",
        "\n",
        "# Clone Repos\n",
        "!rm -rf /content/sample_data\n",
        "!git clone https://github.com/openvpi/DiffSinger.git\n",
        "!git clone https://github.com/openvpi/SingingVocoders\n",
        "\n",
        "# Install dependencies\n",
        "!uv pip install torch torchaudio torchvision --index-url https://download.pytorch.org/whl/cu121\n",
        "!uv pip install click einops h5py librosa lightning matplotlib mido numpy praat-parselmouth preprocessing pyworld PyYAML torchmetrics tqdm tensorboard tensorboardX\n",
        "\n",
        "# Download pretrained models\n",
        "!apt-get install -y aria2\n",
        "if pc_nsf:\n",
        "  !aria2c https://github.com/openvpi/SingingVocoders/releases/download/v1.0.0/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.zip\n",
        "  !mkdir -p /content/SingingVocoders/pretrained\n",
        "  !7z x \"/content/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.zip\" -o/content/SingingVocoders/pretrained\n",
        "  !rm \"/content/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.zip\"\n",
        "else:\n",
        "  !aria2c https://github.com/openvpi/SingingVocoders/releases/download/v0.0.2/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip\n",
        "  !mkdir -p /content/SingingVocoders/pretrained\n",
        "  !7z x \"/content/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip\" -o/content/SingingVocoders/pretrained\n",
        "  !rm \"/content/nsf_hifigan_44.1k_hop512_128bin_2024.02.zip\"\n",
        "\n",
        "#incase theyll add it in the future\n",
        "#!aria2c https://github.com/openvpi/DiffSinger/releases/download/v2.1.0/rmvpe.zip\n",
        "#!7z x /content/rmvpe.zip -o/content/SingingVocoders/pretrained\n",
        "#!rm /content/rmvpe.zip\n",
        "clear_output()\n",
        "print(\"✔️ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocess data for training**"
      ],
      "metadata": {
        "id": "fCfAytC7RCvm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import soundfile as sf\n",
        "import librosa\n",
        "import os\n",
        "import numpy as np\n",
        "import concurrent.futures\n",
        "\n",
        "# Path to zip file containing your audio data\n",
        "data_zip_path = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# Segment interval in seconds\n",
        "segment_interval = 10  # @param {type:\"slider\", min:2, max:60, step:1}\n",
        "\n",
        "train_path = \"/content/audio_data/input\"\n",
        "npz_path = \"/content/audio_data/output\"\n",
        "\n",
        "# Clean up and create directories if needed\n",
        "!rm -rf /content/audio_data >/dev/null 2>&1\n",
        "\n",
        "if not os.path.exists(train_path):\n",
        "    os.makedirs(train_path)\n",
        "    os.makedirs(npz_path)\n",
        "\n",
        "# Extract WAV files from the zip archive\n",
        "!7z e \"$data_zip_path\" -o{train_path} \"*.wav\" -r\n",
        "\n",
        "# Function to resample and segment audio\n",
        "def resample_and_convert_audio(audio_path, sample_rate=44100):\n",
        "    audio, sr = librosa.load(audio_path, sr=None)\n",
        "    duration = librosa.get_duration(y=audio, sr=sr)\n",
        "\n",
        "    if sr != sample_rate:\n",
        "        audio = librosa.resample(y=audio, orig_sr=sr, target_sr=sample_rate)\n",
        "\n",
        "    if duration > segment_interval:\n",
        "        samples_per_segment = segment_interval * sample_rate\n",
        "        total_segments = int(np.ceil(duration / segment_interval))\n",
        "\n",
        "        for segment in range(total_segments):\n",
        "            start_sample = samples_per_segment * segment\n",
        "            end_sample = start_sample + samples_per_segment\n",
        "            if end_sample > len(audio):\n",
        "                end_sample = len(audio)\n",
        "            segment_audio = audio[start_sample:end_sample]\n",
        "\n",
        "            segment_filename = f\"{os.path.splitext(os.path.basename(audio_path))[0]}_segment_{segment}.wav\"\n",
        "            segment_path = os.path.join(os.path.dirname(audio_path), segment_filename)\n",
        "            sf.write(segment_path, segment_audio, sample_rate)\n",
        "        print(f\"Resampled {os.path.basename(audio_path)} to {sample_rate} Hz.\")\n",
        "        print(f\"Segmented {os.path.basename(audio_path)} into {total_segments} parts.\")\n",
        "    else:\n",
        "        sf.write(audio_path, audio, sample_rate)\n",
        "        print(f\"Resampled {os.path.basename(audio_path)} to {sample_rate} Hz.\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    audio_files = []\n",
        "    for root, dirs, files in os.walk(train_path):\n",
        "        for file in files:\n",
        "            if file.endswith(\".wav\"):\n",
        "                audio_files.append(os.path.join(root, file))\n",
        "\n",
        "\n",
        "    with concurrent.futures.ProcessPoolExecutor() as executor:\n",
        "        executor.map(resample_and_convert_audio, audio_files)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "WjKHanCFErik",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Edit Config\n",
        "#@markdown ___\n",
        "\n",
        "import yaml\n",
        "import os\n",
        "import torch\n",
        "#@markdown Model's name and save path\n",
        "exp_name = \"\" # @param {type:\"string\"}\n",
        "save_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "#@markdown Pitch extractor algorithm\n",
        "f0_ext = \"parselmouth\" # @param [\"parselmouth\", \"harvest\"]\n",
        "f0_min = 40 # @param {type:\"slider\", min:0, max:250, step:2}\n",
        "f0_max = 1200 # @param {type:\"slider\", min:800, max:4180, step:20}\n",
        "\n",
        "\n",
        "#@markdown Precision option\n",
        "precision = \"16-mixed\" # @param [\"32-true\", \"bf16-mixed\", \"16-mixed\"]\n",
        "\n",
        "#@markdown Data aug option\n",
        "data_aug = True # @param {type:\"boolean\"}\n",
        "data_aug_probability = 0.5 # @param {type:\"slider\", min:0.1, max:3, step:0.1}\n",
        "\n",
        "#@markdown Amount of validation files you want to use (can't exceed the amount of train files)\n",
        "val_amount = 6 # @param {type:\"slider\", min:1, max:18, step:1}\n",
        "\n",
        "#@markdown Path to the base model for fine tuning | leave blank to use the default ckpt\n",
        "finetune_ckpt_path = \"\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "\n",
        "if finetune_ckpt_path:\n",
        "    finetune_ckpt = finetune_ckpt_path\n",
        "else:\n",
        "  if pc_nsf:\n",
        "    finetune_ckpt = \"/content/SingingVocoders/pretrained/pc_nsf_hifigan_44.1k_hop512_128bin_2025.02.ckpt\"\n",
        "  else:\n",
        "    finetune_ckpt = \"/content/SingingVocoders/pretrained/nsf_hifigan_44.1k_hop512_128bin_2024.02.ckpt\"\n",
        "\n",
        "\n",
        "\n",
        "#@markdown Learning rate of discriminater and generater model\n",
        "learning_rate = 0.00001 # @param {type:\"slider\", min:0.00001, max:0.0005, step:0.00001}\n",
        "\n",
        "with open(\"/content/SingingVocoders/configs/ft_hifigan.yaml\", \"r\") as config:\n",
        "    ew = yaml.safe_load(config)\n",
        "ew[\"data_input_path\"] = [\"/content/audio_data/input\"]\n",
        "ew[\"data_out_path\"] = [save_path + \"/data\"]\n",
        "ew[\"val_num\"] = val_amount\n",
        "ew[\"pe\"] = f0_ext\n",
        "ew[\"f0_min\"] = f0_min\n",
        "ew[\"f0_max\"] = f0_max\n",
        "ew[\"DataIndexPath\"] = save_path\n",
        "ew[\"finetune_ckpt_path\"] = finetune_ckpt\n",
        "ew[\"discriminate_optimizer_args\"][\"lr\"] = learning_rate\n",
        "ew[\"generater_optimizer_args\"][\"lr\"] = learning_rate\n",
        "ew[\"mel_base\"] = \"e\" #for the diffsinger thingy ig\n",
        "if pc_nsf == True:\n",
        "  if torch.cuda.is_available():\n",
        "    device = torch.cuda.current_device()\n",
        "    gpu_name = torch.cuda.get_device_name(device)\n",
        "    if 'A100' or 'Tesla L4':\n",
        "      ew[\"crop_mel_frames\"] = 48\n",
        "      ew[\"batch_size\"] = 10\n",
        "      ew[\"pc_aug_rate\"] = 0.5\n",
        "    if 'Tesla T4':\n",
        "      ew[\"crop_mel_frames\"] = 32\n",
        "      ew[\"batch_size\"] = 10\n",
        "      ew[\"pc_aug_rate\"] = 0.4\n",
        "\n",
        "if pc_nsf == False:\n",
        "  ew[\"pc_aug\"] = False\n",
        "\n",
        "if data_aug:\n",
        "    ew[\"key_aug\"] = data_aug\n",
        "    ew[\"key_aug_prob\"] = data_aug_probability\n",
        "ew[\"pl_trainer_accelerator\"] = \"gpu\"\n",
        "ew[\"pl_trainer_precision\"] = precision\n",
        "with open(\"/content/SingingVocoders/configs/ft_hifigan.yaml\", \"w\") as config:\n",
        "    yaml.dump(ew, config)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"Configs applied!\")"
      ],
      "metadata": {
        "id": "Vs85pHx7bG7E",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Preprocess\n",
        "#@markdown ___\n",
        "\n",
        "%cd /content/SingingVocoders\n",
        "!python3 process.py --config /content/SingingVocoders/configs/ft_hifigan.yaml --strx 1\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "NoE5hWf-l54W",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "c34hWGdmRIWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import os\n",
        "import yaml\n",
        "os.environ['MPLBACKEND'] = 'Agg'\n",
        "\n",
        "%cd /content/SingingVocoders\n",
        "#@title # Training\n",
        "#@markdown ___\n",
        "#@markdown Change config_path to path of the config.yaml for resuming | leave blank for training from scratch\n",
        "config_path = \"\" # @param {type:\"string\"}\n",
        "resume_training = False # @param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Model save interval\n",
        "save_interval = 500 # @param {type:\"slider\", min:100, max:10000, step:100}\n",
        "save_interval = int(save_interval / 2)\n",
        "\n",
        "if config_path:\n",
        "    config_path = config_path\n",
        "else:\n",
        "    config_path = \"/content/SingingVocoders/configs/ft_hifigan.yaml\"\n",
        "\n",
        "training_utils_path = \"/content/SingingVocoders/utils/training_utils.py\"\n",
        "with open(training_utils_path, \"r\") as f:\n",
        "    edit_relative_path = f.read()\n",
        "new_relative = \"relative_path = filepath.relative_to(Path('/content').resolve())\"\n",
        "pattern = r\"relative_path\\s*=\\s*.*\"\n",
        "edit_relative_path = re.sub(pattern, new_relative, edit_relative_path)\n",
        "with open(training_utils_path, \"w\") as f:\n",
        "    f.write(edit_relative_path)\n",
        "\n",
        "with open(config_path, \"r\") as config:\n",
        "    bitch = yaml.safe_load(config)\n",
        "bitch[\"val_check_interval\"] = save_interval #questionable\n",
        "with open(config_path, \"w\") as config:\n",
        "    yaml.dump(bitch, config)\n",
        "\n",
        "if resume_training:\n",
        "    exp_name = os.path.basename(os.path.dirname(config_path))\n",
        "    save_path = os.path.dirname(os.path.dirname(config_path))\n",
        "    log = save_path + \"/\" + exp_name\n",
        "else:\n",
        "    log = save_path + \"/\" + exp_name\n",
        "\n",
        "logdir = log\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir {logdir}\n",
        "!python3 train.py --config {config_path} --exp_name {exp_name} --work_dir {save_path}"
      ],
      "metadata": {
        "cellView": "form",
        "id": "s_zvFSHo5V8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ONNX export for OU usage**"
      ],
      "metadata": {
        "id": "50QqVlRzRKFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "%cd /content/SingingVocoders\n",
        "# Paths\n",
        "ckpt_path = \"\" # @param {type:\"string\"}\n",
        "ckpt_folder = os.path.dirname(ckpt_path)\n",
        "ckpt_config = ckpt_folder + \"/config.yaml\"\n",
        "name = \"\" # @param {type:\"string\"}\n",
        "export_path = \"\" # @param {type:\"string\"}\n",
        "save_path =  export_path + \"/model.ckpt\"\n",
        "\n",
        "!uv pip install -r /content/DiffSinger/requirements-onnx.txt\n",
        "clear_output()\n",
        "# Export!\n",
        "!python3 export_ckpt.py --ckpt_path {ckpt_path} --save_path {save_path}\n",
        "\n",
        "!python3 /content/DiffSinger/scripts/export.py nsf-hifigan \\\n",
        "    --config \"{ckpt_config}\" \\\n",
        "    --name \"{name}\" \\\n",
        "    --ckpt \"{save_path}\" \\\n",
        "    --out \"{ckpt_folder}\"\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "37tYasasB8hH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}